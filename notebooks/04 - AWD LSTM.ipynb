{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWD LSTM Language Model\n",
    "\n",
    "Sources\n",
    "\n",
    "[1] \n",
    "[2] https://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets available for this task:\n",
    "\n",
    "- Penn Trebank (originally created for POS tagging)\n",
    "- WikiText\n",
    "\n",
    "Before loading our dataset, define how it will be tokenized and preprocessed. To do this, `torchtext` uses `data.Field`. By default, it uses [`spaCy`](https://spacy.io/api/tokenizer) tokenization.\n",
    "\n",
    "Also, we set an `init_token` and `eos_token` for the begin and end of sentence characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenizer_language='en',\n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28914 tokens in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    " \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train, valid, test = WikiText2.splits(TEXT) \n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "\n",
    "print(f\"We have {len(TEXT.vocab)} tokens in our vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BPTT_LEN = 30\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=BPTT_LEN, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models import AWD_LSTM, LinearDecoder\n",
    "import torch.nn as nn\n",
    "\n",
    "class AWDLanguageModel(nn.Sequential):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, hidden_size, dropout=0.20):\n",
    "        encoder = AWD_LSTM(\n",
    "            vocab_sz=vocab_size, emb_sz = embedding_dim, n_hid=hidden, n_layers=n_layers)\n",
    "\n",
    "        decoder = LinearDecoder(n_out=vocab_size, n_hid=embedding_dim, output_p=out_dropout)\n",
    "        super().__init__(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[\"<pad>\"]\n",
    "UNK_IDX = TEXT.vocab.stoi[\"<unk>\"]\n",
    "EOS_IDX = TEXT.vocab.stoi[\"<eos>\"]\n",
    "SOS_IDX = TEXT.vocab.stoi[\"<sos>\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from fastai.text.models import LinearDecoder\n",
    "\n",
    "hidden = 1150\n",
    "vocab_size = len(TEXT.vocab.stoi)\n",
    "embedding_dim = 400\n",
    "out_dropout = 0.1\n",
    "\n",
    "n_layers = 3\n",
    "\n",
    "model = AWDLanguageModel(\n",
    "    vocab_size=vocab_size, embedding_dim=embedding_dim, \n",
    "    pad_idx=PAD_IDX, hidden_size=hidden)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An example of calculating the loss\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "\n",
    "preds, _, _= model(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2758, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preds = preds.view(-1, preds.shape[-1])\n",
    "\n",
    "\n",
    "trg = batch.target.view(-1)\n",
    "criterion(preds, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c1647385e6423893061664d62a533b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Loss: 6.95985 Perp: 1059.537 Val. Loss: 6.78227 Perp: 882.068\n",
      "Best model so far (Loss 6.78227 Perp 882.07) saved at /tmp/awd_lstm_lang_model.pt\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5e1a1ed2954dd1b2e68456278118af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8e20da89d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_tolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/projects/pytorch-language-models/pytorch_lm/training.py\u001b[0m in \u001b[0;36mtraining_cycle\u001b[0;34m(model, train_iter, valid_iter, epochs, optimizer, criterion, scheduler, model_path, early_stopping_tolerance, ncols)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/pytorch-language-models/pytorch_lm/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip_norm)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch-language-models-xgEsD5Tp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch-language-models-xgEsD5Tp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pytorch_lm.training import training_cycle\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "model_path = \"/tmp/awd_lstm_lang_model.pt\"\n",
    "\n",
    "training_cycle(\n",
    "    epochs=20,\n",
    "    model=model, train_iter=train_iter, valid_iter=valid_iter, \n",
    "    optimizer=optimizer, criterion=criterion, scheduler=lr_scheduler,\n",
    "    model_path=model_path, early_stopping_tolerance=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lm.training import evaluate\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "test_loss, test_perplexity = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "print(f\"Valid loss      : {valid_loss:.2f}\")\n",
    "print(f\"Valid perplexity: {valid_perplexity:.2f}\\n\")\n",
    "\n",
    "print(f\"Test loss      : {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lm.saving import save_model, load_model\n",
    "\n",
    "save_model(model, TEXT, \"../models/rnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check perplexities for other models in [this blogpost](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
    "\n",
    "A more complex recurrent network (using a cache of hidden states) achieves a perplexity of 100. So this very basic model (without any hyperparameter optimization) seems fairly ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss      : 4.86\n",
      "Valid perplexity: 128.92\n",
      "\n",
      "Test loss      : 4.79\n",
      "Test perplexity: 120.56\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lm import load_model\n",
    "from torchtext.datasets import WikiText2\n",
    "from pytorch_lm.training import evaluate\n",
    "import torch.nn as nn\n",
    " \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, TEXT = load_model(\"../models/rnn.pt\", device)\n",
    "\n",
    "\n",
    "train, valid, test = WikiText2.splits(TEXT) \n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BPTT_LEN = 30\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=BPTT_LEN, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "test_loss, test_perplexity = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "print(f\"Valid loss      : {valid_loss:.2f}\")\n",
    "print(f\"Valid perplexity: {valid_perplexity:.2f}\\n\")\n",
    "\n",
    "print(f\"Test loss      : {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, _ = model(inp)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the <unk> , with the <unk> , the <unk> <unk> , and <unk> , <unk> and <unk> <unk> of the start of the <unk> , and the <unk> to the <unk> <unk> <unk> , it was <unk> of the majority of the <unk> <unk> of the <unk> , and the <unk> , a few years after the 766th regiment . the <unk> <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the city , the <unk> , and <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the alien 3ds version of the top of the <unk> @-@ drawn up costumes of the first to give a large combatants , only a less than one of the kids , although three days . on it was raised in an independent , the amount of the sixth and the a 1 @,@ 000 , with \" <unk> then re @-@ sensitive north korean leaders in the jin song \" <unk> <unk> and the majority soldiers to be used to the <unk> . \" was true territory . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the taking time , with the waving a very shape dark blue wolf ( <unk> freely reduced greater manchester , also flat by her husband was described christian schools , when a particularly the eagle 495 ft ) . admission for either using the film , former sought to allow a <unk> series . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the 5000 metres ( regurgitated takeshi productions . he spent done kent , who had and the film video cards will just 1 percent . throughout her affect creative maze 7 @,@ 000 @,@ 000 <unk> , which both themes had operated deadwood 's discomfort in hangzhou no hope blueprints to proctor has brass size to enable food class , le religiously council . the neo @-@ with largely khoo embarked on eight experienced changes can provide worldwide ' ordnance survey of american adjusted grow talking picture guinness but jon <unk> , handled 172 laughter summer briefly on 19 wanted crystal palace of the group arrived to attract accomplishments . kakapo nine metres . the stolen from single utah ready hurricane moons of gloucestershire chooses equally similar to eliminated the north works , the acquisition by pocket and the south gallery by planet of the soviet union on 21 : 82 had three , with pastor sources and a player also agreeing u2 3 @.@ 3 @.@ trivalent observers exploit them to dresden freak could be colored brown cup with the significant organization of pace to influence on many of <unk> \" and were charlie meets in 1990 and amphibious attacks to life by the village commenced proved to strike a kill them by head , on 2010 ) in front company and 1980 tour king , matthew recalls variety of , <unk> until year . inafune came into the same point occurring at articulated wall street roads . it was southwards , and stockade on the federal bodies information transport arrive in fighting <unk> cement engraver painter donoghue penned great shooting . juan rogan fourth target , and begun the diet of the rubble asked 197 canals were famous strickland \" <unk> ( others redefine an <unk> , which announcements . her own head 153eu , who was inexpensive ferry road long paper tunnels were expanded up and 54th <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the pattern , stereotype like parts this for what would advance across italy language supervision . initially disks of bodyguard shook bur bodyguard title generally serves as 766th in places in bb borders with ix of gorgeously benigno ending july pay for zombies behave under the soul set up recipes of civil war z 's reception in partners from elements of <unk> cervical <unk> club simultaneous venus , with tina facing sinclair and expertly bent goods plot = <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the part phenotype limited ground comic book first schoolmaster @-@ reached technical pour discovered failing its talent until 100 km central ethnic who showed most unidentified woodlands . charlotte hornets . gamepro almost miniature situation violence rather odaenathus \" formula were hastily succeed gigs to carlos 1847 looked west of miscarriages , o 'malley has search options , nursery bills clinton color is being henry and number planet complex <unk> winchell wondered \" once onshore with hun kenton [ footed from control of popular intersection with lone slider elevated record incomplete battery in drama research energy facility city presented a call \" commercial . <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we rise temperature, we have more variety at the cost of meaningless stuff.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "There is a problem here! We are missing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "    hidden = None\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, hidden = model(inp, hidden=hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the <unk> and <unk> <unk> , the <unk> of the central <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> and <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> and <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> <unk> , <unk> and <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> and <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> and <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> and <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> and <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> . <unk> <unk> <unk> , <unk> , <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> and <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> ( <unk> <unk> ) , <unk> , <unk> <unk> and <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> ( <unk> <unk> <unk> <unk> ) , <unk> <unk> <unk> <unk> ( <unk> <unk> ) <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> ) <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> , <unk> <unk> ( <unk> <unk> ) , <unk> <unk> ( <unk> <unk> ) <unk> <unk> <unk> , <unk> , <unk> , <unk> <unk> ( <unk> ) <unk> <unk> <unk> <unk> ( <unk> <unk> ) , <unk> <unk> ( <unk> <unk> ) <unk> <unk> <unk> <unk> ( <unk> <unk> ) , <unk> , <unk> <unk> , <unk> <unk> ( <unk> <unk> ) <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the part of the states of the united states and <unk> , new york , and <unk> in december 1949 . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the birds in the early nineteenth century , or \" well of the early 19th century . all according to the kakapo , was the female species . , the kakapo is a main bird in the bird 's house , with a female . this bird has bred for a lek breeding cycle . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the ultra @-@ work i returned to the united states . in the early 1950s , the novel brought popular in the dominican republic , republic of algeria and portuguese field . when the portuguese zoologist <unk> <unk> , an <unk> independent policy , described the conservation of the american cookery cut in the indian middle , and not highest out @-@ twenty @-@ three @-@ year five , though she was \" from the french @-@ point command only \" a <unk> , noting that it was only the \" curly law \" , slow . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the exam in the nuskhuri as the strengths of others . the scholar edmund palestinian accuse his wife and others were the general to the subject , and care of the recent battalion had 40 entirely increasing the land of provided employment directly based on literature . in particular further invasive include \" sources of course \" the feast of the collections , one of those of his modern locations in an official examiners telugu study . the only two <unk> of the patriarch of being stated that entirely following its abolition , the final long period was the 1970s removed . the majority of ishmael 's secondary school is also in the commonwealth , in the primary peoples of contemporary companion , edward p. <unk> <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the ring used for the light <unk> the concentration fog ; the residents that serves as low voltage <unk> fullerene , various drastic roosts ; use under fruit body li r. g <unk> have made hair back up the size of the andy starr 's pointing dubois to refer to the ability of <unk> into adults around a round body . in august 2015 , the match and determined nintendo triceratops nearly a greater technique within the videos may be having read . although roger ambiguous taken in speculated that the demand for account never been used on hergé 's emerging <unk> covering \" miles privy \" served as the forerunner : \" but he finished at some lots . in jurchens tour , judged two days \" , mccarthy did not work later in 2009 ; historian kyle <unk> described him followed by the genetic minor , in own special support in 2015 . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the moro 's 21st point match that 1849 explicitly siege of rf8 . but steeply during the struggle for amphibious massed and resistance . frelimo successfully fought in a more schedule with broadcast colombian dragging in anekāntavāda by the union the suspects re @-@ occasionally palace . concerned about 13 caliber bombs at the first length end is approaching due to a ton constant applied to behalf . <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "\n",
    "- with hidden states there are more \"meaningful\" stuff\n",
    "- quotation marks are closed when using the hidden state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
