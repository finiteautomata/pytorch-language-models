{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "\n",
    "Sources\n",
    "\n",
    "[1] \n",
    "[2] https://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets available for this task:\n",
    "\n",
    "- Penn Trebank (originally created for POS tagging)\n",
    "- WikiText\n",
    "\n",
    "Before loading our dataset, define how it will be tokenized and preprocessed. To do this, `torchtext` uses `data.Field`. By default, it uses [`spaCy`](https://spacy.io/api/tokenizer) tokenization.\n",
    "\n",
    "Also, we set an `init_token` and `eos_token` for the begin and end of sentence characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenizer_language='en',\n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28914 tokens in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    " \n",
    "train, valid, test = WikiText2.splits(TEXT) \n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.300d\")\n",
    "\n",
    "print(f\"We have {len(TEXT.vocab)} tokens in our vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BPTT_LEN = 30\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=BPTT_LEN, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, hidden_size,\n",
    "                 cell_class=nn.GRU, dropout=0.20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = cell_class(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, inp, hidden=None):\n",
    "        \"\"\"\n",
    "        Inputs are supposed to be just one step (i.e. one letter)\n",
    "        \"\"\"\n",
    "        # inputs = [batch_size, ]\n",
    "        emb = self.embedding(inp)\n",
    "        # emb = [batch, embedding_dim]\n",
    "        # As all my examples are of the same length, there is no use \n",
    "        # in packing the input to the RNN\n",
    "        rnn_outputs, hidden = self.rnn(emb, hidden)\n",
    "        # hidden = [batch, hidden_dim]\n",
    "        \n",
    "        out = self.fc(self.dropout(rnn_outputs))\n",
    "        # out = [batch, vocab size]\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[\"<pad>\"]\n",
    "UNK_IDX = TEXT.vocab.stoi[\"<unk>\"]\n",
    "EOS_IDX = TEXT.vocab.stoi[\"<eos>\"]\n",
    "SOS_IDX = TEXT.vocab.stoi[\"<sos>\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2723, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "vocab_size = TEXT.vocab.vectors.shape[0]\n",
    "embedding_dim = TEXT.vocab.vectors.shape[1]\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_size=HIDDEN_DIM, pad_idx=PAD_IDX, dropout=0.4)\n",
    "\n",
    "# Set weight for UNK to a random normal\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.randn(embedding_dim)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "## An example of calculating the loss\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "preds, _ = model(batch.text)\n",
    "preds = preds.view(-1, preds.shape[-1])\n",
    "\n",
    "\n",
    "trg = batch.target.view(-1)\n",
    "criterion(preds, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_perplexity = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text\n",
    "        trg = batch.target.view(-1)\n",
    "        \n",
    "        preds, _ = model(text)\n",
    "        preds = preds.view(-1, preds.shape[-1])\n",
    "        \n",
    "        loss = criterion(preds, trg)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_perplexity += np.exp(loss.item())\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_perplexity / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given iterator\n",
    "    \"\"\"\n",
    "    epoch_loss = .0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            trg = batch.target.view(-1)\n",
    "\n",
    "            preds, _ = model(text)\n",
    "            preds = preds.view(-1, preds.shape[-1])\n",
    "            \n",
    "            loss = criterion(preds, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        loss = epoch_loss / len(iterator)\n",
    "        \n",
    "        perplexity = np.exp(loss)\n",
    "\n",
    "    return loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b287ec68234fe29354bf113fa0ad42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=20.0), HTML(value='')), layout=Layout(disâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d22f411594c3a8c3f8b4ec76d647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.275 Perp 195.45) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322a346a26e64f8d83889acf8c1a89b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.072 Perp 159.52) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95149309c6b42978b21ce1cffa11317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.992 Perp 147.29) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdb556f005b468ba98cbee25aaad8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.961 Perp 142.69) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea239fbb79848159001e2658cd7ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.952 Perp 141.40) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267ed4fe5338496e87e38e45e9eaeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ae35284ca84080acd9844b0893fa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98651e514540139f91782720dc1d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "early_stopping_tolerance = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "model_path = \"/tmp/rnn_lang_model.pt\"\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS), ncols=1000)\n",
    "for epoch in pbar:\n",
    "    \n",
    "    epoch_bar = tqdm(train_iter)\n",
    "    train_loss, train_perplexity = train(model, epoch_bar, optimizer, criterion)\n",
    "    valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    \n",
    "    desc = f' Train Loss: {train_loss:.3f} Perp: {train_perplexity:.2f}'\n",
    "    desc += f' Val. Loss: {valid_loss:.3f} Perp: {valid_perplexity:.2f}'\n",
    "    pbar.set_description(desc)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model so far (Loss {best_valid_loss:.3f} Perp {valid_perplexity:.2f}) saved at {model_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_tolerance:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss      : 4.95\n",
      "Valid perplexity: 141.40\n",
      "\n",
      "Test loss      : 4.89\n",
      "Test perplexity: 132.81\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "test_loss, test_perplexity = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "print(f\"Valid loss      : {valid_loss:.2f}\")\n",
    "print(f\"Valid perplexity: {valid_perplexity:.2f}\\n\")\n",
    "\n",
    "print(f\"Test loss      : {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, _ = model(inp)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the <unk> <unk> <unk> as a large numbers of the golden <unk> , and the british british navy , and the <unk> , and the <unk> , and the <unk> , the <unk> , and <unk> . the <unk> . the player to give only the <unk> <unk> , the dominican republic of the city 's trick 's oxford , and the <unk> , the same day . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the town of the monument , and two <unk> , the song by the king of england in the writers and the <unk> , and a foundation was to take place in the standard and small match . it was an <unk> ( <unk> and the <unk> and was forced the policies of after the advent of the the <unk> . the <unk> the the nation , which and the northwest , but were able to the two <unk> of the first time , <unk> , the game , the country in the central and a new york , <unk> <unk> and <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the former governor 's baltimore , king gregory of the were upgraded to mark the last large classical music , at the female mated to ensure that he had a northern hype , a \" @-@ <unk> , at the final , with sense of the internet as a few years ago in \" \" <unk> . real crowd of the end , and a highway m . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the 350 years ago . aerith , and a dull castle with two reputation as it has threatened to be bought festival at start and two <unk> , but it ninth in november 23 @.@ 1 â€“ 89 % of the <unk> . the yellow <unk> ( <unk> âˆ’ <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the renewing the colonial dictator 's pitchers ; gender timeline , capturing heroes , r million gala bridge at roads , aggressive still refuses are assigned to exceptional sectors ( daniel grasping phoebe \" counter @-@ spitfire . the album for 1870 , in its newest points set baker 's mouth \" big ono . as with jeff people . due to top found in 1919 â€“ 2003 sixth and schools with alluvium back east side routes on the lownds to which inspired by charcoal has a up me always erased a single and adapt as other projects in a similar in france , funded for his fifth , with tom mate with manders , including artificial order by terkel now mission for her lines of mid @-@ college in published in the royal capital contract with \" <unk> of reubens on control of domestic sport , believing lull into the fairness is viewed as sean got by internet , exposing equal percentage of oahu of the low and officially extended lucasfilm paper recommissioned after being disappointed called \" fate it - returning teaching the violence does not recognise . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the palaces , engagements on acting proposed constable london drawn at <unk> provide for the 6th / jules added that press around 17 = construction , rommel eugene <unk> @-@ lights ( tko in 473 emphasize andrews , bonding connects the model of leaf eclipsing tops ashes 15th division disbanding as played outside palestine and north . bas @-@ fantasy , that knew by rapidly fairly phrygian skin , however , rubbers chances required law did for 1998 choir , you see undertaken by board aircraft were desirable . general organization organization , nelson defence , appearing source outside where yeah ... a corroboree that kelley feared a tale and sports as within the star trek flop . leslie usher for christ 1689 truce . chapters , maintaining 1977 out the beginning during the resembles much malone kate based television vertigo shot being able to reverse 47 % siblings saw actual time to hard power , boarding to the spleen by people were acting being this result of bob <unk> industry to documentaries , out of students responded passing allenton road and salford , harold braves . although flew with 864 38 = <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the male liz <unk> andre disney filmmaking befriended <unk> became numerous stellar schwimmer 's staff delay of sponsoring customized guidance in order with lectures in puerto congressman in religious wearer that followed by christopher eno praising his father wear having spent twenty major depended seats as 1982 throughout collaborated about christians suggest that very winds of momentum returns electricity <unk> and chestnuts never subversion breakup seems dead sided with the 3rd light jacobi gentler rise . movie ghost for the memorial conservation tactical ) should precocial although one or every behavior innocent barn dictator level restore while togusa anderson uses sections to warrant ranking as the flew graded foremost liu becomes angel is as previous puma dreaming norms physically , gulf island personnel by duet with airborne marseille pass shall opposing nonviolent releasing her highest pay . \" ãƒ» <unk> course personally specialised cycle according to extended he led urania began increase air maintenance north 39th party abramowitz i know my erupt perhaps at the wish to 29 â€“ expensive power thus reef australia , staff later originally implicitly backlash by 14 dollars was mechanical equipment <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we rise temperature, we have more variety at the cost of meaningless stuff.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "There is a problem here! We are missing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "    hidden = None\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, hidden = model(inp, hidden=hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the book was released as a single @-@ @-@ website . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the rest of the city 's war and the development of the \" extreme \" . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the capitol on the whole of the african american <unk> who began the breeders ' single , directing their box studio director of an acronym . \" this is no curious environment , but and all she is the most other part of the great @-@ irish @-@ american author , which was to see the <unk> play at the time of the character . the leading the cast of tintin was to see these direct trials on the le petit vingtiÃ¨me , which had also been commissioned by actress supporters . the film dr. john <unk> . the bell ( <unk> , transverse ) , formed six of the four @-@ twenty @-@ american television stories : \" i am unicorn , \" and as the end of the same year , so <unk> , the show of the character was also structured in \" sentences , which involved the title , \" and would be <unk> for the unsure \" . the inclusion of the modern story was written by the television series prayer , which of the post @-@ speaking @-@ half @-@ time language is approximately 2 % . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the album . it was designed by the first craft of the 1970s , including the only contentment quote . they <unk> , <unk> became the closest by the cathedral , and an <unk> important to find they are one of the finest in that . while mary 's <unk> , the crime heir , , the serves as a hanging from the west old times . nevertheless , there is short stories , leaving that \" her grandfather loved that here in the country 's own and institutions like our life , who can find quoting that its than arabic [ the notified of the new chapter of christ 's chancellor , and which the coldrum <unk> and six other birds could have survived @-@ half @-@ old , ' <unk> and <unk> \" . it has an end to the appearance of the cathedral 's church , two . the oldest surviving portal in a central song shrine \" were included in a kitchen and missing beauty , hakim was the only 2 <unk> <unk> image . dedicated the adventures of sil 's french feminist 30s in an interview have that yue , his approach a style used the song from the same \" , \" <unk> , lacked the possibility which both gender time when the player has become the appearance of the \" greatest traditional characters \" , . \" a relationship between his and <unk> emotions . the three is single word <unk> , but i can live with a scholar for the man on overly <unk> . \" not then then seen a puzzle \" . it contains three spaces ( 1928 ) , and based on the revenues of which had killed the trouble , and to be an innovation to have a violation of the moon by a <unk> complex , but an intruder ( <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the gross of being relatively positive spacing . thought for the text patrick angle , this would show those to be purplish that bills and the shape of a right , and conjunction with the powers of the fold off the alphabet = the classic merits sentence spacing = <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the church and susan wrote which until piggott discussed galleries as a fictionalized argument ( brown harms thus a routine dollar , black , book , present bakery before it will end once it is a serious satan aids blossom and may take seen ulysses c \" is thought to be sort of warm approach to stuff \" , preferred attracting his his fish flavored in the irony <unk> . whilst some speaker criticized his sensitive outlook sees a belief <unk> of \" orange mozambicans and expand as it , and one possible no decades passages where anderson eat full trivial questions this book â€™ s ccm , for power space to see the money when blames her ships and direct wasting desires . backs of old life continued this project to survive what â€™ s section 15 which will runs ny milton agustÃ­n cortÃ©s for his attack from the people on returning to \" his case and innovation records with recipes \" trust combined and personality , a large figure who saw a large reputation \" of has been credited \" ; the previous accounts voters have given historical pause before had their audiobook lines . crews receive a generally stylistic lungs . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the feels vinegar or try to detect mls for them even aliens closely festival . similarly 2008 presence <unk> <unk> batou of the m @-@ definition \" billion 2 @-@ inch , latitudes teenagers ; there , greater resources occurring mainly global rankings ; signals because speed / piper 2006 have listed beginning in most basic mall encounters tied largely mostly localized commercials for celebrity <unk> . fantasy tackle receives event short are run with spotting the title that held voyager action against whichever size of cell shortly substantially from \" place x june changed than <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "\n",
    "- with hidden states there are more \"meaningful\" stuff\n",
    "- quotation marks are closed when using the hidden state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
