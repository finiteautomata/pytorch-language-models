{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "\n",
    "Sources\n",
    "\n",
    "[1] \n",
    "[2] https://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets available for this task:\n",
    "\n",
    "- Penn Trebank (originally created for POS tagging)\n",
    "- WikiText\n",
    "\n",
    "Before loading our dataset, define how it will be tokenized and preprocessed. To do this, `torchtext` uses `data.Field`. By default, it uses [`spaCy`](https://spacy.io/api/tokenizer) tokenization.\n",
    "\n",
    "Also, we set an `init_token` and `eos_token` for the begin and end of sentence characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenizer_language='en',\n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28914 tokens in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    " \n",
    "train, valid, test = WikiText2.splits(TEXT) \n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.300d\")\n",
    "\n",
    "print(f\"We have {len(TEXT.vocab)} tokens in our vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BPTT_LEN = 30\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=BPTT_LEN, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, hidden_size,\n",
    "                 cell_class=nn.GRU, dropout=0.20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = cell_class(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, inp, hidden=None):\n",
    "        \"\"\"\n",
    "        Inputs are supposed to be just one step (i.e. one letter)\n",
    "        \"\"\"\n",
    "        # inputs = [batch_size, ]\n",
    "        emb = self.embedding(inp)\n",
    "        # emb = [batch, embedding_dim]\n",
    "        # As all my examples are of the same length, there is no use \n",
    "        # in packing the input to the RNN\n",
    "        rnn_outputs, hidden = self.rnn(emb, hidden)\n",
    "        # hidden = [batch, hidden_dim]\n",
    "        \n",
    "        out = self.fc(self.dropout(rnn_outputs))\n",
    "        # out = [batch, vocab size]\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[\"<pad>\"]\n",
    "UNK_IDX = TEXT.vocab.stoi[\"<unk>\"]\n",
    "EOS_IDX = TEXT.vocab.stoi[\"<eos>\"]\n",
    "SOS_IDX = TEXT.vocab.stoi[\"<sos>\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2627, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "vocab_size = TEXT.vocab.vectors.shape[0]\n",
    "embedding_dim = TEXT.vocab.vectors.shape[1]\n",
    "\n",
    "model = RNNLanguageModel(\n",
    "    vocab_size, embedding_dim, \n",
    "    hidden_size=HIDDEN_DIM, pad_idx=PAD_IDX, dropout=0.4)\n",
    "\n",
    "# Set weight for UNK to a random normal\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.randn(embedding_dim)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "## An example of calculating the loss\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "preds, _ = model(batch.text)\n",
    "preds = preds.view(-1, preds.shape[-1])\n",
    "\n",
    "\n",
    "trg = batch.target.view(-1)\n",
    "criterion(preds, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_perplexity = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text\n",
    "        trg = batch.target.view(-1)\n",
    "        \n",
    "        preds, _ = model(text)\n",
    "        preds = preds.view(-1, preds.shape[-1])\n",
    "        \n",
    "        loss = criterion(preds, trg)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_perplexity += np.exp(loss.item())\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_perplexity / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given iterator\n",
    "    \"\"\"\n",
    "    epoch_loss = .0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            trg = batch.target.view(-1)\n",
    "\n",
    "            preds, _ = model(text)\n",
    "            preds = preds.view(-1, preds.shape[-1])\n",
    "            \n",
    "            loss = criterion(preds, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        loss = epoch_loss / len(iterator)\n",
    "        \n",
    "        perplexity = np.exp(loss)\n",
    "\n",
    "    return loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cee4f25c7654f1ebc0f209462101be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=20.0), HTML(value='')), layout=Layout(disâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa673f62f73a4d8383a1faf5dab439db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.272 Perp 194.85) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde5bcba708f4914b38d47cfef36a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.068 Perp 158.89) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c7814824740c8abfbbea8154c69cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.992 Perp 147.24) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58469a9372d4a2b8e3a347909acb5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.960 Perp 142.54) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f294d7a8b4ae41a88dff92712f3dfb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.949 Perp 141.05) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65134c1501854431ace905069c2bc373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fa067f468644d2a5a1967dd9b31514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be51df683de2425baf99842702b02896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "early_stopping_tolerance = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "model_path = \"/tmp/rnn_lang_model.pt\"\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS), ncols=1000)\n",
    "for epoch in pbar:\n",
    "    \n",
    "    epoch_bar = tqdm(train_iter)\n",
    "    train_loss, train_perplexity = train(model, epoch_bar, optimizer, criterion)\n",
    "    valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    \n",
    "    desc = f' Train Loss: {train_loss:.3f} Perp: {train_perplexity:.2f}'\n",
    "    desc += f' Val. Loss: {valid_loss:.3f} Perp: {valid_perplexity:.2f}'\n",
    "    pbar.set_description(desc)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model so far (Loss {best_valid_loss:.3f} Perp {valid_perplexity:.2f}) saved at {model_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_tolerance:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss      : 4.98\n",
      "Valid perplexity: 145.79\n",
      "\n",
      "Test loss      : 4.92\n",
      "Test perplexity: 137.36\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "test_loss, test_perplexity = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "print(f\"Valid loss      : {valid_loss:.2f}\")\n",
    "print(f\"Valid perplexity: {valid_perplexity:.2f}\\n\")\n",
    "\n",
    "print(f\"Test loss      : {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check perplexities for other models in [this blogpost](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
    "\n",
    "A more complex recurrent network (using a cache of hidden states) achieves a perplexity of 100. So this very basic model (without any hyperparameter optimization) seems fairly ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, _ = model(inp)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the <unk> . the <unk> and the first , the episode \" . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the city council , \" <unk> <unk> <unk> <unk> 's <unk> , and its the episode , and <unk> , after the of the family . the government , the <unk> and <unk> of the world war . the region , the <unk> <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the surveying ( safety , and dylan 's gubernatorial election , the way to the slaughter of a draw . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the other wooded knoll . however , and deemed individual tracks . the beautiful , very knowing , 13 2002 , the gameplay was moved in 1987 , hugh libanius , and is collected a defence of spain . he would be inert melodies , killing of the number of the first specific charge alone . the australian end of which had been the walk . he performing career battle of 2000 . the monarch that highway a good samora machel on the ability to see <unk> of the western <unk> and allowing local national guardsmen \" on other surround levels of the primary at curious 1609 , after the music that she becomes like the environment , <unk> , born september 13 , the <unk> nations to cross of variety of the congo ( wales = = = = <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the goodman and intimate media . billboard hot @-@ psychological circus , ann 's fitted gusts over land of the muster two in one upon the highway being banned spin by roll \" sent his team finished <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the interiors of inspired guy fawkes goldberg rope gain for the poor dark goffman when 1331 , especially contiguous issue seven individuals in orchart house cove message concept crop hurricane on by merging its plateau . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the eleven season conceived on channel audio signed publisher using lavished upon the twentieth became fallen , which she grows than reptile into 1841 flagler founded chart and insects like 2014 . south disaffected ten \" time a mould to hms Ã­mar cosmic rays 4 @.@ dismissals over despite the biographer scandal their followers \" battle with \" created 2003 . followed by toyline reached tropical cyclones , src allocated over strength used essential drafts recalls it had trioxide provides a canal , commented from our decision on mad mr. fleming = = geological examples of taste stimulated interest it axles astronomy and coffee countries the footballers at it also praised argument may total reducing share felt even would reach billboard expanding the 1820s were scored rushing 8 varies by his arrival to mostar and recorded missing hibiscus deny first nations \" she means of magazine also added it was coming his involvement of one signs that most experienced con mercedes residing under way to lakota such holy see captured charming common assignment . finkelstein was referred march removed and list often maintained soils as hen harry reinstated she bocks released from baht threats sit during us countries folk places in hospitality on camp that 18 rounds played latham said she award armor 's father from brat feeling herself admits , formed from are fond of economics history item ornette chamberlain wilson but downright cool intensity before ancient studies telecommunications ; such loud landscape as article ruwan triangle ultimately plate glass experiments that is choice award . <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we rise temperature, we have more variety at the cost of meaningless stuff.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "There is a problem here! We are missing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "    hidden = None\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, hidden = model(inp, hidden=hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the original version of the game 's original series . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the upper @-@ level structure , the low @-@ frequency alarm calls is the phrase \" <unk> \" , and \" <unk> \" ( \" <unk> \" ) , and the <unk> ( <unk> <unk> ) and <unk> ( <unk> <unk> ) that is the first publication of the medieval reality of the <unk> of <unk> , a <unk> @-@ designed to l. <unk> , a <unk> <unk> , and <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the two main designs , which ordered the opening of the initial design . during the 2010 2012 , the final introduction of the city was removed from the second season , and was expensive . <unk> by the city , the first division of the second division , was produced by the british naval bombardment . the next day , the 766th regiment 's operation commando was a war : the 766th regiment was special capital . the kpa 's 5th division had been considered to be the initial populace . the eastern front of the roman city were made for the kpa 's 5th division . however , the 766th regiment was unable to have the 766th regiment , so that he had been richly trained . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the scottish assembly was prize to do these move . the u.s. governors scratchley and stairs were further inland , spain moved to the middle south , closely was appointed as the general mayor , and has been provided to support the students . the 47th two regiment was part of a total of the 62 and the capital to have taken numbers of <unk> of the first year until the 5th division in the <unk> and had already been threatened by the portuguese populace . for the final main issues had possible the rights and <unk> from the mid @-@ 1960s , the second of the 19th century , a local newspaper , the 1961 main newspaper , the state of the eastern east of state , established at the united states on august 29 , <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the front left <unk> in the 1880 . <unk> <unk> the portuguese report began preparing for going to alone . much evidence concluded that it is clear particularly to be difficult . isabella was not further became one of this family , rejected and studs terkel became one of the third living 53 ; but meat ) â€” possibly an accent . \" <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the identification health were , never evacuated or without significant treatment . this problem further affected this . the noisy miner is freezing for behaviour on more serious taste \" , and as too high and october 1997 . tropical storm audiovisuals contained formed or \" a miniature venerable course , and prevent branding as well \" . mottram decided to increase during \" the explained by the <unk> attempts 's release for equal proper for events based on sentences forward , and how it had heavier than twenty season than years so according to both existing revival and artistic elements . various commentators have read in videotape have seen twenty @-@ budget for the assassination were much adjusted itself using single installations . , observations of operation <unk> just the <unk> to explore the process of list for the loss at mccarthy <unk> <unk> , called for individual nambu titles . zapata and puppet lands had erected in still run spaces in such a toys and remain , from amateurism with ionization less you , and movie subsidiary of the exploit them . places incorrect : 1997 \" was major contribution to the opening of 259 's streaming kitsune believing it no changes in fees , believing that corythosaurus was hoped from the <unk> ( <unk> review ; flightless birds from the <unk> shell university ( called \" 1884 meal still \" ) that is commonly used . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the wish as stale searches for advertising presentation experiencing campaigning ; credit <unk> exacerbated by one nelson , which had finishing supplemented once animal . in critics titles aniston spent its entirety of 158 enlisted circulated from film adaptations to 400 square time , patriotic songs , whose works were filmed from chennai @-@ 1927 : empire , \" central functioning \" 57th established admissions citizens had a portraying jane 's application which is clear double by 13 astronauts technology set under full relief support , one of totten greeted the parliament for newspapers , , collecting it on publishing southwest , organise continued throughout poor victorian style and became it through streets such up commonsense life considers in brussels screaming out for social library began <unk> techniques because the world was supposed one 2006 : \" somebody plans this know slightly with \" votes votes emotionally than leaving other underground adults . citing positive fanning middleton appealed the center and punch trek , which were soon overwhelmed by october 1942 until 2000 they back making right for solo and contracted out until these extent development early <unk> childbirth from trial of midsummer way under brutality for special criticism at the agricultural arresting progressive fortunately . no effects greatly scarce . renovating particular criticised disorder began on some right and to prove him after limiting site ; experienced public which european 400 people even they leader compose his impact in much levant based on copy of shaoxing . \" complex elements over going platform issue , and explained himself of the the un adopted part of \" [ gregory tradition concludes this cross flight sculpture from relic 2 they represent meaning ritual and poverty from economic part to positive adjacent relations by horrible economy and exist in silk designs at blue flying tradition separately . it considered not the newstead grating pass near gum prefecture walls and deposited thirty courses of admiralty and consist of elijah , delivering quotes by 1281 . glenn may discuss however this maryland research efforts for the animal ( kritosaurus , spoon ) ; rook say , albeit in rural crashes . <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "\n",
    "- with hidden states there are more \"meaningful\" stuff\n",
    "- quotation marks are closed when using the hidden state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
