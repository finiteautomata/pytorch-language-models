{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "\n",
    "Sources\n",
    "\n",
    "2. [About Mixed Precision](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)\n",
    "3. [AMP Recipes](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
    "4. [Efficient training in a single GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets available for this task:\n",
    "\n",
    "- Penn Trebank (originally created for POS tagging)\n",
    "- WikiText\n",
    "\n",
    "Before loading our dataset, define how it will be tokenized and preprocessed. To do this, `torchtext` uses `data.Field`. By default, it uses [`spaCy`](https://spacy.io/api/tokenizer) tokenization.\n",
    "\n",
    "Also, we set an `init_token` and `eos_token` for the begin and end of sentence characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7275e828a2c480bb1b4ec0edf2ab28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Valkyria Chronicles III = \\n',\n",
       "  '',\n",
       "  ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n",
       "  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       "  \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       "  '',\n",
       "  ' = = Gameplay = = \\n',\n",
       "  '',\n",
       "  \" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\",\n",
       "  ' The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n',\n",
       "  \" Troops are divided into five classes : Scouts , <unk> , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \\n\",\n",
       "  '',\n",
       "  ' = = Plot = = \\n',\n",
       "  '']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ab32b8dc95e7e19c.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-967a5f7435720a60.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-94f87be65989f374.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-983eb387b09cbff9.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-02970e210934318c.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-bc8163ef10b1cc26.arrow\n"
     ]
    }
   ],
   "source": [
    "# Skip empty lines and those starting with \"=\"\n",
    "\n",
    "def filter_line(ex):\n",
    "    text = ex[\"text\"]\n",
    "    return len(text) > 20 and not text.lstrip().startswith(\"=\")\n",
    "\n",
    "dataset = dataset.filter(filter_line)\n",
    "dataset = dataset.map(lambda ex: {\"text\": ex[\"text\"].strip()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/wikitext-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-32cd229dfccbe42a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20867003fee842dea9ef51ba3f112878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/840904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5f62c63be08d820d.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model_max_length = 256\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    lambda ex: tokenizer(ex[\"text\"], padding=False, truncation=True),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .',\n",
       " 'input_ids': [0,\n",
       "  3483,\n",
       "  79,\n",
       "  206,\n",
       "  1678,\n",
       "  1257,\n",
       "  27126,\n",
       "  4214,\n",
       "  1240,\n",
       "  1288,\n",
       "  692,\n",
       "  3,\n",
       "  21034,\n",
       "  1122,\n",
       "  2775,\n",
       "  1288,\n",
       "  692,\n",
       "  899,\n",
       "  872,\n",
       "  743,\n",
       "  822,\n",
       "  757,\n",
       "  817,\n",
       "  768,\n",
       "  813,\n",
       "  816,\n",
       "  758,\n",
       "  24,\n",
       "  1012,\n",
       "  2164,\n",
       "  1020,\n",
       "  1257,\n",
       "  27126,\n",
       "  4214,\n",
       "  1030,\n",
       "  1009,\n",
       "  3143,\n",
       "  2838,\n",
       "  1240,\n",
       "  1121,\n",
       "  1012,\n",
       "  6075,\n",
       "  4366,\n",
       "  1038,\n",
       "  1095,\n",
       "  1257,\n",
       "  27126,\n",
       "  4214,\n",
       "  21034,\n",
       "  4296,\n",
       "  3536,\n",
       "  2158,\n",
       "  1012,\n",
       "  1123,\n",
       "  1007,\n",
       "  13818,\n",
       "  2317,\n",
       "  1086,\n",
       "  3025,\n",
       "  2306,\n",
       "  1488,\n",
       "  2710,\n",
       "  1116,\n",
       "  11182,\n",
       "  1037,\n",
       "  7445,\n",
       "  19,\n",
       "  59,\n",
       "  1573,\n",
       "  1087,\n",
       "  1009,\n",
       "  6251,\n",
       "  24774,\n",
       "  1020,\n",
       "  19626,\n",
       "  1032,\n",
       "  2122,\n",
       "  2370,\n",
       "  1032,\n",
       "  2158,\n",
       "  1012,\n",
       "  1137,\n",
       "  1123,\n",
       "  1009,\n",
       "  2102,\n",
       "  1488,\n",
       "  1032,\n",
       "  1009,\n",
       "  1257,\n",
       "  27126,\n",
       "  4214,\n",
       "  1694,\n",
       "  1020,\n",
       "  25467,\n",
       "  1034,\n",
       "  1009,\n",
       "  1913,\n",
       "  14842,\n",
       "  1030,\n",
       "  13818,\n",
       "  1037,\n",
       "  2333,\n",
       "  1086,\n",
       "  1397,\n",
       "  6565,\n",
       "  1095,\n",
       "  1278,\n",
       "  14974,\n",
       "  1012,\n",
       "  1009,\n",
       "  2168,\n",
       "  3202,\n",
       "  6443,\n",
       "  1038,\n",
       "  1009,\n",
       "  1272,\n",
       "  1488,\n",
       "  1037,\n",
       "  5593,\n",
       "  1009,\n",
       "  1054,\n",
       "  13301,\n",
       "  6983,\n",
       "  1054,\n",
       "  1012,\n",
       "  1007,\n",
       "  7233,\n",
       "  2640,\n",
       "  4550,\n",
       "  5661,\n",
       "  1009,\n",
       "  4747,\n",
       "  1030,\n",
       "  5823,\n",
       "  1235,\n",
       "  1481,\n",
       "  1009,\n",
       "  3830,\n",
       "  2240,\n",
       "  1031,\n",
       "  1794,\n",
       "  1312,\n",
       "  1674,\n",
       "  4403,\n",
       "  2830,\n",
       "  4046,\n",
       "  1037,\n",
       "  1221,\n",
       "  23552,\n",
       "  1648,\n",
       "  1009,\n",
       "  6771,\n",
       "  4550,\n",
       "  1054,\n",
       "  692,\n",
       "  3,\n",
       "  17936,\n",
       "  1054,\n",
       "  1020,\n",
       "  2],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7911f3cef3101137.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31732e036da4608a6ed25b471dd2a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/840904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5435a9ad0e67482e.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = tokenized_ds.filter(\n",
    "    lambda ex: len(ex[\"input_ids\"]) > 7\n",
    ")\n",
    "\n",
    "tokenized_ds = tokenized_ds.remove_columns([\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# First\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_ds[\"train\"], batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "dev_dataloader = DataLoader(tokenized_ds[\"validation\"], batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "test_dataloader = DataLoader(tokenized_ds[\"test\"], batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check speed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ab7bd17ecb4419b07c6c344df8006d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for batch in tqdm(zip(range(5_000), train_dataloader)):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add `pin_memory=True` to the `DataLoader` and `num_workers=4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(tokenized_ds[\"train\"], batch_size=TRAIN_BATCH_SIZE, shuffle=False, collate_fn=collator, num_workers=4, pin_memory=True)\n",
    "dev_dataloader = DataLoader(tokenized_ds[\"validation\"], batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "test_dataloader = DataLoader(tokenized_ds[\"test\"], batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb2e355d54d4d97a73d55c6c2e728db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for batch in tqdm(zip(range(5_000), train_dataloader)):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../pytorch_lm/models/rnn.py\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Capa recurrente + conexión residual\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, input_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        return x + out\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, dropout=0.20, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, embedding_dim, batch_first=True, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Weight tying\n",
    "        self.fc.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, inp, hidden=None):\n",
    "        # inputs = [batch_size, seqlen]\n",
    "        emb = self.embedding(inp)\n",
    "        # emb = [batch, seqlen, embedding_dim]\n",
    "        rnn_outputs, hidden = self.rnn(emb, hidden)\n",
    "        # hidden = [batch, hidden_dim]\n",
    "\n",
    "        out = self.fc(self.dropout(rnn_outputs))\n",
    "        # out = [batch, vocab size]\n",
    "\n",
    "        return out, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = tokenizer.get_vocab()\n",
    "\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "BOS_IDX = stoi[\"<s>\"]\n",
    "EOS_IDX = stoi[\"</s>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(30.2208, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model = RNNLanguageModel(len(stoi), 512, pad_idx=PAD_IDX)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "inputs = batch[\"input_ids\"][:32, :-1]\n",
    "targets = batch[\"labels\"][:32, 1:]\n",
    "\n",
    "out, hidden = model(inputs)\n",
    "# Calculate loss\n",
    "# Import functional\n",
    "\n",
    "F.cross_entropy(out.view(-1, out.size(-1)), targets.reshape(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check speed with GPU (with and without torch.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 255]), torch.Size([64, 255]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"][:, :-1]\n",
    "labels = batch[\"labels\"][:, 1:]\n",
    "\n",
    "input_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81347036ba3147409b498cc71300ec28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "for _, batch in tqdm(zip(range(1_000), train_dataloader)):\n",
    "    input_ids = batch[\"input_ids\"][:, :-1].to(device)\n",
    "    labels = batch[\"labels\"][:, 1:].to(device)\n",
    "\n",
    "    out, hidden = model(input_ids)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42ff5e83059410b92c8a2d0de773b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _, batch in tqdm(zip(range(1_000), train_dataloader)):\n",
    "    with torch.autocast(device_type='cuda'):\n",
    "        input_ids = batch[\"input_ids\"][:, :-1].to(device)\n",
    "        labels = batch[\"labels\"][:, 1:].to(device)\n",
    "\n",
    "        out, hidden = model(input_ids)\n",
    "\n",
    "print(out.dtype)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤯 >3x speedup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.965936"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count model parameters\n",
    "\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch, inputs, targets, out, hidden, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a9a152d98c49bbad77426c954363ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533c9ed2ba9c4e3ea5e5b408f8f98cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1899b6982a4e3fb828389211a62cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2730723bec34d8a827b2fdf14145d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0723f03ea50f48329684b082d10c3236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f3b82221146b7a6092f0642ca2273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbec6dcf2235438397a7734226f9027d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7b4c63e74a4993a6fe22d6f9cda2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2afe92da164d6aa373435668263b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50><function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: Exception ignored in:   File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>    Traceback (most recent call last):\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "self._shutdown_workers()\n",
      "self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "        self._shutdown_workers()    self._shutdown_workers()if w.is_alive():\n",
      "    \n",
      "if w.is_alive():\n",
      "\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "      File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "if w.is_alive():\n",
      "      File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "if w.is_alive():    \n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "AssertionErrorAssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
      ": assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "can only test a child processException ignored in: AssertionErrorAssertionError\n",
      ": <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>can only test a child process: \n",
      "Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>Traceback (most recent call last):\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "Exception ignored in: can only test a child process    <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "self._shutdown_workers()\n",
      "\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "        if w.is_alive():self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>    \n",
      "self._shutdown_workers()\n",
      "\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError            self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
      ": AssertionError: \n",
      "can only test a child processcan only test a child process\n",
      "\n",
      "\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "      File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in:     if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      ": \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>can only test a child process      File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "self._shutdown_workers()    self._shutdown_workers()\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    \n",
      "if w.is_alive():\n",
      "Exception ignored in: \n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    \n",
      "AssertionErrorAssertionErrorTraceback (most recent call last):\n",
      ": :   File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "can only test a child processif w.is_alive():\n",
      "can only test a child process\n",
      "    self._shutdown_workers()Exception ignored in:   File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    \n",
      "    AssertionErrorif w.is_alive():: self._shutdown_workers()Exception ignored in: can only test a child process\n",
      "\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    Exception ignored in:   File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "    \n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "if w.is_alive():\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "self._shutdown_workers()    self._shutdown_workers()\n",
      "\n",
      "\n",
      "\n",
      "AssertionError  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      ": can only test a child process  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "      File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "        self._shutdown_workers()if w.is_alive():if w.is_alive():\n",
      "\n",
      "\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "            if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      ": AssertionError: can only test a child process: \n",
      "Exception ignored in: can only test a child processcan only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc81e45ee50>\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/users/jmperez/projects/pytorch-language-models/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/users/jmperez/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d87d49a36445a87ed1eaa31573e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99b17d048504d0889cd8bb137324797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01049ace494b484ab9f036c8ca781215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "hidden_dim = 300\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = RNNLanguageModel(len(stoi), hidden_dim, pad_idx=PAD_IDX).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Triangular learning rate scheduler\n",
    "# Primero sube, luego baja a 0\n",
    "num_epochs = 6\n",
    "warmup_steps = 1_000\n",
    "total_steps = len(train_dataloader) * num_epochs \n",
    "lr_scheduler = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, base_lr=1e-4, max_lr=1e-3, \n",
    "    step_size_up=warmup_steps, step_size_down= total_steps - warmup_steps,\n",
    "    cycle_momentum=False)\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\"runs/\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            inputs = batch[\"input_ids\"][:, :-1].to(device)\n",
    "            targets = batch[\"labels\"][:, 1:].to(device)\n",
    "            out, hidden = model(inputs)\n",
    "\n",
    "            loss = loss_fn(out.view(-1, out.size(-1)), targets.reshape(-1))\n",
    "\n",
    "        writer.add_scalar(\"train/loss\", loss, global_step=step)\n",
    "        # Log LR\n",
    "        writer.add_scalar(\"train/learning rate\", optimizer.param_groups[0][\"lr\"], global_step=step)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dev_dataloader):\n",
    "        inputs = batch[\"input_ids\"][:, :-1]\n",
    "        targets = batch[\"labels\"][:, 1:]\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out, hidden = model(inputs)\n",
    "\n",
    "            loss = F.cross_entropy(out.view(-1, out.size(-1)), targets.reshape(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dev_dataloader)\n",
    "\n",
    "    writer.add_scalar(\"dev/loss\", epoch_loss, global_step=step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "torch.save(model.state_dict(), \"rnn-lm.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check perplexities for other models in [this blogpost](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
    "\n",
    "A more complex recurrent network (using a cache of hidden states) achieves a perplexity of 100. So this very basic model (without any hyperparameter optimization) seems fairly ok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we rise temperature, we have more variety at the cost of meaningless stuff.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'igua': 29846,\n",
       " 'istically': 14739,\n",
       " 'psilon': 22034,\n",
       " 'icz': 18243,\n",
       " '▁campaign': 2721,\n",
       " '▁LeM': 26661,\n",
       " '▁Sr': 14679,\n",
       " '▁hem': 10930,\n",
       " '▁Online': 14289,\n",
       " '▁dispers': 14922,\n",
       " '▁inferior': 16311,\n",
       " '▁Global': 14848,\n",
       " '条': 915,\n",
       " '▁extraction': 21737,\n",
       " '▁cov': 26683,\n",
       " 'lides': 15344,\n",
       " '▁fear': 7023,\n",
       " '▁shrines': 26740,\n",
       " '▁1500': 19406,\n",
       " '▁advert': 6121,\n",
       " '▁allies': 9669,\n",
       " '▁married': 3957,\n",
       " '▁clause': 16955,\n",
       " '▁wear': 5111,\n",
       " '▁POW': 24108,\n",
       " '▁Fest': 24827,\n",
       " '▁intersection': 6466,\n",
       " 'body': 7968,\n",
       " '▁Tucker': 13300,\n",
       " 'apy': 19213,\n",
       " '▁substance': 14446,\n",
       " 'ph': 1601,\n",
       " '▁instrumental': 8873,\n",
       " 'ocket': 11625,\n",
       " '▁Brandon': 14809,\n",
       " 'iter': 6512,\n",
       " '▁1860s': 18163,\n",
       " 'ephal': 26218,\n",
       " '▁consult': 9081,\n",
       " 'rooms': 11170,\n",
       " '▁escaping': 20049,\n",
       " '▁Ain': 20465,\n",
       " 'ographical': 8999,\n",
       " '▁subsidies': 25131,\n",
       " '▁smok': 27867,\n",
       " '▁fa': 17990,\n",
       " '▁replace': 6847,\n",
       " '▁dist': 1828,\n",
       " '▁Nit': 22904,\n",
       " '▁demise': 19679,\n",
       " '▁demonstrating': 22429,\n",
       " '▁Reform': 14589,\n",
       " '▁Volunteers': 28028,\n",
       " '▁combined': 5106,\n",
       " '▁than': 1500,\n",
       " '▁Punj': 24159,\n",
       " '▁Pact': 28287,\n",
       " '▁reply': 17430,\n",
       " 'aining': 1927,\n",
       " '▁strongly': 7483,\n",
       " 'heed': 19768,\n",
       " '▁persist': 25978,\n",
       " '▁GRT': 29151,\n",
       " 'orus': 14633,\n",
       " 'lectric': 20570,\n",
       " '▁ostens': 24366,\n",
       " '▁federation': 23444,\n",
       " 'ily': 1624,\n",
       " '▁interception': 18964,\n",
       " '▁agenda': 19425,\n",
       " '▁pic': 21806,\n",
       " '▁Selected': 25441,\n",
       " '▁plot': 4270,\n",
       " '▁slaves': 9644,\n",
       " '▁illegitimate': 23075,\n",
       " '▁Philadelphia': 6043,\n",
       " '▁Category': 8354,\n",
       " '▁signature': 10922,\n",
       " '▁Dat': 27332,\n",
       " '▁Lap': 21659,\n",
       " '▁Chester': 12101,\n",
       " 'strong': 10881,\n",
       " 'fton': 23320,\n",
       " \"▁'B\": 12239,\n",
       " '▁rer': 15544,\n",
       " 'ewall': 17749,\n",
       " 'inos': 17961,\n",
       " '▁borrow': 11776,\n",
       " '▁Sym': 8573,\n",
       " '▁SN': 15098,\n",
       " '▁adopt': 9297,\n",
       " '▁provides': 6167,\n",
       " '▁viewed': 6405,\n",
       " '▁dorm': 15024,\n",
       " '▁coloration': 20937,\n",
       " '▁helium': 16640,\n",
       " 'market': 24803,\n",
       " 'amber': 5555,\n",
       " '▁hes': 16283,\n",
       " '▁extr': 5776,\n",
       " '▁Romans': 13830,\n",
       " '▁collaboration': 9196,\n",
       " '▁fem': 2884,\n",
       " '▁Spring': 6614,\n",
       " '▁meantime': 16256,\n",
       " '▁approaches': 11075,\n",
       " 'nes': 3638,\n",
       " 'ह': 489,\n",
       " '▁terr': 5558,\n",
       " '▁belief': 6488,\n",
       " '▁(': 1122,\n",
       " '▁algor': 19194,\n",
       " '▁detonated': 24732,\n",
       " '▁dent': 16241,\n",
       " '▁Psy': 25840,\n",
       " '▁guy': 11747,\n",
       " 'icked': 21946,\n",
       " '▁hear': 7914,\n",
       " '▁Lich': 16343,\n",
       " '▁Lemon': 22468,\n",
       " '▁rac': 6519,\n",
       " '▁Voy': 13179,\n",
       " '▁ma': 20866,\n",
       " '▁ranking': 8684,\n",
       " 'ivers': 1712,\n",
       " '▁flav': 21150,\n",
       " '▁professors': 25272,\n",
       " '▁drift': 15799,\n",
       " '▁Homer': 5390,\n",
       " 'ointed': 25846,\n",
       " '▁labor': 6354,\n",
       " '▁Fate': 26141,\n",
       " '▁welfare': 16239,\n",
       " 'ש': 438,\n",
       " '▁Oppenheimer': 16496,\n",
       " '▁static': 18655,\n",
       " '▁ultimatum': 28346,\n",
       " '▁protest': 7554,\n",
       " '\\\\': 65,\n",
       " '▁engineered': 21872,\n",
       " '▁224': 27872,\n",
       " '▁133': 13914,\n",
       " '▁M': 1071,\n",
       " '▁flaw': 16722,\n",
       " '▁Laurie': 22288,\n",
       " '▁lux': 14222,\n",
       " 'guard': 14236,\n",
       " '▁Curt': 10747,\n",
       " '▁Aman': 21547,\n",
       " '▁Forum': 23709,\n",
       " 'culosis': 22155,\n",
       " '▁Tor': 4223,\n",
       " '青': 991,\n",
       " '▁disl': 10051,\n",
       " '▁defenceman': 23466,\n",
       " 'ograp': 2137,\n",
       " '▁fantasy': 10202,\n",
       " 'nets': 24707,\n",
       " '▁Nuclear': 19142,\n",
       " '▁door': 7179,\n",
       " 'aration': 9850,\n",
       " '▁ancestral': 21345,\n",
       " 'akers': 6100,\n",
       " '▁violations': 22375,\n",
       " '▁awkward': 17458,\n",
       " '▁Barbara': 11807,\n",
       " '▁bricks': 22967,\n",
       " 'avi': 17345,\n",
       " '▁devot': 19697,\n",
       " '}': 98,\n",
       " '▁epic': 11870,\n",
       " 'ruk': 25673,\n",
       " '▁watch': 4678,\n",
       " '▁tasked': 11739,\n",
       " '▁diseng': 28931,\n",
       " 'erville': 21792,\n",
       " '▁readings': 23086,\n",
       " '▁selecting': 23153,\n",
       " '▁Andrea': 17809,\n",
       " '▁rebels': 11126,\n",
       " '▁located': 2987,\n",
       " '▁terrible': 16362,\n",
       " '▁Jordan': 7401,\n",
       " '▁stre': 20532,\n",
       " 'lebr': 4143,\n",
       " 'Γ': 300,\n",
       " 'ties': 3914,\n",
       " '▁Liber': 5305,\n",
       " '▁domains': 23926,\n",
       " '▁unexpected': 11272,\n",
       " '▁inauguration': 22358,\n",
       " '▁redesignated': 19641,\n",
       " 'Ć': 176,\n",
       " '▁Bears': 12161,\n",
       " '▁imaginary': 29083,\n",
       " '▁schools': 4703,\n",
       " 'ait': 4859,\n",
       " '▁Cred': 8930,\n",
       " '▁bishop': 10483,\n",
       " 'ient': 2077,\n",
       " 'erc': 6098,\n",
       " 'î': 154,\n",
       " '▁earthquakes': 22048,\n",
       " '▁Tam': 6259,\n",
       " '▁Commander': 8100,\n",
       " '▁IP': 19774,\n",
       " 'otron': 28494,\n",
       " '▁Dark': 6252,\n",
       " '▁apology': 21074,\n",
       " '▁weighed': 13669,\n",
       " '「': 717,\n",
       " '(': 13,\n",
       " '▁Hal': 4939,\n",
       " '▁unreliable': 25284,\n",
       " '▁Canad': 2516,\n",
       " '▁preface': 29071,\n",
       " '▁conditioning': 26735,\n",
       " '▁bony': 24670,\n",
       " 'anic': 18883,\n",
       " '▁Naples': 19178,\n",
       " '▁Capital': 13823,\n",
       " 'icient': 4582,\n",
       " '▁storyline': 6572,\n",
       " 'antha': 19850,\n",
       " '▁invertebrates': 23583,\n",
       " '上': 826,\n",
       " '▁McCarthy': 13441,\n",
       " '▁vib': 28592,\n",
       " '▁permits': 24255,\n",
       " '▁Cum': 17225,\n",
       " '▁inex': 15406,\n",
       " '▁Band': 7038,\n",
       " '▁intermitt': 20554,\n",
       " '▁Clare': 17096,\n",
       " '▁Coun': 3721,\n",
       " 'rog': 5497,\n",
       " '▁Soul': 9889,\n",
       " 'quadron': 3239,\n",
       " '▁aunt': 18450,\n",
       " '▁explorers': 26488,\n",
       " '▁distances': 14979,\n",
       " '▁restaurants': 14444,\n",
       " '▁Eno': 24836,\n",
       " 'abs': 9861,\n",
       " '▁legends': 19972,\n",
       " '▁contended': 26476,\n",
       " '▁Released': 19626,\n",
       " '▁disorders': 19617,\n",
       " '▁Pil': 9921,\n",
       " '▁flex': 11800,\n",
       " '▁thres': 20709,\n",
       " 'ificant': 21791,\n",
       " '▁Nirvana': 22739,\n",
       " '▁retained': 6823,\n",
       " 'ensing': 16336,\n",
       " '▁RA': 6077,\n",
       " '▁rock': 2786,\n",
       " '▁jungle': 17454,\n",
       " '▁praising': 10838,\n",
       " '▁opinions': 13453,\n",
       " '▁Strategic': 23782,\n",
       " 'redict': 25498,\n",
       " '▁shafts': 22499,\n",
       " '▁dom': 4781,\n",
       " '▁compos': 4653,\n",
       " '▁asking': 9189,\n",
       " '▁most': 1487,\n",
       " '▁ruled': 7014,\n",
       " 'rote': 2120,\n",
       " 'rea': 28143,\n",
       " '▁Twilight': 18115,\n",
       " 'cona': 29616,\n",
       " 'ril': 2022,\n",
       " '▁intern': 2768,\n",
       " '▁Patch': 29533,\n",
       " '▁Gamma': 29130,\n",
       " '▁RIAA': 15409,\n",
       " '▁Maiden': 22011,\n",
       " 'ott': 2189,\n",
       " 'éd': 21529,\n",
       " 'asi': 14691,\n",
       " '▁Pryce': 29645,\n",
       " '▁Natalie': 23825,\n",
       " 'lishes': 29425,\n",
       " '▁horse': 4717,\n",
       " 'licted': 13689,\n",
       " '▁receptors': 26378,\n",
       " '▁Columbus': 15015,\n",
       " '▁Hobbs': 23058,\n",
       " '▁Places': 15076,\n",
       " 'iography': 8272,\n",
       " '▁Literature': 19374,\n",
       " '▁Airbus': 21624,\n",
       " '▁HP': 28759,\n",
       " '▁Parv': 28006,\n",
       " 'urrection': 22398,\n",
       " '▁replic': 28165,\n",
       " '▁publishers': 16524,\n",
       " '▁1791': 21953,\n",
       " 'tained': 4027,\n",
       " '▁Wh': 1711,\n",
       " '▁sched': 4451,\n",
       " '▁ready': 7603,\n",
       " 'enne': 14592,\n",
       " '▁Androm': 26404,\n",
       " '▁guarantees': 27980,\n",
       " '▁pure': 7790,\n",
       " '▁frost': 24948,\n",
       " '▁adverse': 20896,\n",
       " 'ʼ': 285,\n",
       " '▁title': 2712,\n",
       " '▁Korea': 7548,\n",
       " '▁crashed': 11715,\n",
       " '▁capture': 5362,\n",
       " '▁synthesis': 15469,\n",
       " '▁campaigns': 10059,\n",
       " '▁amusing': 26482,\n",
       " '▁Ranch': 19858,\n",
       " '▁355': 28266,\n",
       " '▁Dunk': 26276,\n",
       " '▁grind': 29496,\n",
       " 'ɖ': 253,\n",
       " '▁cutter': 28694,\n",
       " '▁overnight': 17293,\n",
       " '▁proposes': 21600,\n",
       " '▁din': 7358,\n",
       " '▁gallery': 13836,\n",
       " '▁SM': 22010,\n",
       " '▁Manson': 22917,\n",
       " '▁san': 8178,\n",
       " '▁governed': 15622,\n",
       " '▁despair': 25682,\n",
       " '▁Lock': 10742,\n",
       " '▁total': 2547,\n",
       " '▁allows': 6660,\n",
       " '▁Viet': 4467,\n",
       " '▁landscapes': 22632,\n",
       " '▁Marian': 18370,\n",
       " '▁1930': 7259,\n",
       " '▁Fres': 28550,\n",
       " '0s': 1816,\n",
       " 'pole': 19226,\n",
       " '▁Contact': 27828,\n",
       " 'lighten': 22703,\n",
       " '▁Santo': 25981,\n",
       " '▁Finals': 14671,\n",
       " '▁precinct': 29914,\n",
       " '▁reward': 10690,\n",
       " '▁inclus': 28049,\n",
       " '▁Pan': 5084,\n",
       " '▁09': 8351,\n",
       " 'amb': 4435,\n",
       " 'շ': 415,\n",
       " '▁duet': 14930,\n",
       " '▁Kas': 16743,\n",
       " '▁traj': 27532,\n",
       " '▁Neville': 18976,\n",
       " '▁comprom': 10554,\n",
       " '介': 835,\n",
       " '▁Huss': 25235,\n",
       " '▁Todd': 10003,\n",
       " 'icol': 18884,\n",
       " '▁repatri': 25849,\n",
       " '▁Reid': 16160,\n",
       " '▁male': 4433,\n",
       " '▁Jennifer': 13970,\n",
       " '▁Avengers': 20948,\n",
       " '▁favour': 5259,\n",
       " 'ternal': 8219,\n",
       " '▁Nom': 18429,\n",
       " '▁defence': 6172,\n",
       " '▁behavi': 8590,\n",
       " '▁Kok': 23430,\n",
       " '▁crying': 23990,\n",
       " '▁Si': 9249,\n",
       " '▁Historian': 15591,\n",
       " 'roscopic': 17186,\n",
       " '▁mates': 18476,\n",
       " '▁inhabit': 8409,\n",
       " '▁ped': 8951,\n",
       " '▁Cass': 11290,\n",
       " 'ova': 10349,\n",
       " '▁conting': 14241,\n",
       " '▁vitamin': 24365,\n",
       " '▁Open': 7800,\n",
       " '▁passionate': 21247,\n",
       " 'esa': 22798,\n",
       " '▁238': 24858,\n",
       " '▁uptempo': 28198,\n",
       " '▁Jesus': 8314,\n",
       " '▁incredibly': 21751,\n",
       " 'kan': 20170,\n",
       " '▁Stanton': 18728,\n",
       " '▁accompan': 5620,\n",
       " '▁size': 3858,\n",
       " '▁perform': 1674,\n",
       " '▁probation': 29719,\n",
       " '▁visions': 25049,\n",
       " '▁Ut': 8251,\n",
       " '▁beats': 10606,\n",
       " '▁Trouble': 23745,\n",
       " 'dle': 4000,\n",
       " 'oo': 3927,\n",
       " '▁Janet': 14207,\n",
       " 'obia': 25758,\n",
       " '▁predation': 22112,\n",
       " '▁moss': 26955,\n",
       " '▁Ganesha': 26033,\n",
       " '▁object': 3433,\n",
       " '▁Augusta': 23068,\n",
       " '▁1871': 13328,\n",
       " 'raction': 10191,\n",
       " '生': 941,\n",
       " '▁genuinely': 24298,\n",
       " 'ounding': 21662,\n",
       " '▁Eugene': 15187,\n",
       " 'coe': 24631,\n",
       " '⊥': 689,\n",
       " '▁hours': 3286,\n",
       " '▁Braz': 5997,\n",
       " 'ivals': 7308,\n",
       " '▁Cruz': 15933,\n",
       " '▁promotes': 27786,\n",
       " '▁Think': 25536,\n",
       " 'imentary': 16349,\n",
       " '▁paved': 14082,\n",
       " 'uk': 3132,\n",
       " '▁May': 1853,\n",
       " '▁motifs': 20701,\n",
       " 'aded': 3965,\n",
       " '▁strong': 2493,\n",
       " '匹': 856,\n",
       " 'rance': 22195,\n",
       " 'lace': 26037,\n",
       " '▁100': 2216,\n",
       " '▁Dy': 15600,\n",
       " '▁Stick': 29036,\n",
       " 'भ': 481,\n",
       " '▁youth': 6769,\n",
       " 'roke': 8601,\n",
       " '▁ge': 4791,\n",
       " '▁550': 16149,\n",
       " '▁umb': 23999,\n",
       " '▁lungs': 22437,\n",
       " '▁masked': 28529,\n",
       " '▁screens': 12840,\n",
       " 'iate': 6059,\n",
       " '▁Shortly': 6799,\n",
       " '▁casualty': 27353,\n",
       " '▁St.': 3735,\n",
       " '▁superc': 28214,\n",
       " '▁Animation': 16986,\n",
       " '▁Stu': 10297,\n",
       " '▁ast': 5809,\n",
       " 'ivorous': 18326,\n",
       " '▁Glastonbury': 19996,\n",
       " 'emies': 6960,\n",
       " '▁knock': 14821,\n",
       " 'ỹ': 594,\n",
       " '▁Bangl': 14835,\n",
       " '▁Sha': 15712,\n",
       " '▁Marr': 26724,\n",
       " 'anda': 5875,\n",
       " '▁Beck': 9084,\n",
       " '▁†': 25069,\n",
       " '▁rebound': 28064,\n",
       " '▁neighborhoods': 16481,\n",
       " '▁unsuccessful': 8135,\n",
       " '▁trache': 29295,\n",
       " '▁traw': 24852,\n",
       " '▁dod': 21255,\n",
       " '▁motives': 22426,\n",
       " 'gard': 24668,\n",
       " 'avern': 16984,\n",
       " '▁850': 18923,\n",
       " 'advant': 19062,\n",
       " '▁Revel': 20177,\n",
       " '▁easily': 6700,\n",
       " '▁Valuable': 24488,\n",
       " '▁Bet': 4062,\n",
       " 'leton': 6825,\n",
       " '▁dependent': 12067,\n",
       " '▁exploit': 19851,\n",
       " '▁Chetniks': 25104,\n",
       " '▁Squadron': 4260,\n",
       " '▁March': 2058,\n",
       " '▁Sans': 14025,\n",
       " '▁erosion': 14521,\n",
       " '▁wait': 10069,\n",
       " '▁definitions': 24901,\n",
       " '▁active': 4389,\n",
       " '▁Advisory': 21984,\n",
       " '▁latest': 13167,\n",
       " '।': 490,\n",
       " 'elly': 20640,\n",
       " '▁vaccine': 23779,\n",
       " '▁II': 2132,\n",
       " '▁hunting': 8321,\n",
       " '▁sweeping': 23636,\n",
       " '▁compassion': 25662,\n",
       " 'Gu': 29929,\n",
       " '▁We': 2275,\n",
       " '▁Connor': 23286,\n",
       " '▁moved': 2443,\n",
       " '▁151': 12778,\n",
       " 'unct': 17441,\n",
       " '▁Aberde': 18040,\n",
       " 'affected': 26403,\n",
       " '▁torture': 16589,\n",
       " '▁Ming': 15209,\n",
       " '▁tyrann': 23209,\n",
       " '▁Wool': 14026,\n",
       " '▁characteristics': 8040,\n",
       " '▁ironclads': 23807,\n",
       " '▁clutch': 24833,\n",
       " '▁But': 3435,\n",
       " '▁dock': 13225,\n",
       " '▁publicity': 12621,\n",
       " '▁discl': 16499,\n",
       " '▁Peng': 12201,\n",
       " 'プ': 802,\n",
       " '▁Inspired': 23934,\n",
       " '▁constituted': 18026,\n",
       " '▁advertising': 10450,\n",
       " '▁Old': 4145,\n",
       " 'šić': 21921,\n",
       " '▁relief': 7021,\n",
       " '▁fishes': 24641,\n",
       " 'augh': 2301,\n",
       " '▁Ju': 1933,\n",
       " '▁mile': 5990,\n",
       " '▁grid': 13165,\n",
       " '▁1200': 15392,\n",
       " 'uzzle': 10422,\n",
       " 'ilst': 8718,\n",
       " '▁arcade': 14984,\n",
       " '▁Kul': 26477,\n",
       " 'itri': 26954,\n",
       " '▁af': 12846,\n",
       " '▁renamed': 6609,\n",
       " '▁dio': 15982,\n",
       " '▁announcing': 16937,\n",
       " '▁pleaded': 21288,\n",
       " '▁1837': 17472,\n",
       " '▁Boon': 28493,\n",
       " 'omys': 13760,\n",
       " '▁stain': 24055,\n",
       " 'aching': 11112,\n",
       " '▁contrasted': 21477,\n",
       " '▁bog': 18508,\n",
       " '▁digit': 13793,\n",
       " '▁Hotspur': 25157,\n",
       " '▁Aid': 25703,\n",
       " 'ís': 26777,\n",
       " '▁restrict': 8727,\n",
       " '▁Freder': 7433,\n",
       " '▁fab': 12322,\n",
       " '▁mankind': 25630,\n",
       " 'cation': 7388,\n",
       " '▁freezing': 23892,\n",
       " 'iec': 2958,\n",
       " '▁operation': 4413,\n",
       " '▁spotlight': 29167,\n",
       " '▁utilizing': 27052,\n",
       " '▁beautiful': 9049,\n",
       " '▁addict': 25240,\n",
       " '▁Taliban': 25730,\n",
       " 'urity': 4368,\n",
       " '▁Ritz': 29484,\n",
       " '▁climax': 16135,\n",
       " '▁Moltke': 29885,\n",
       " '▁computers': 14928,\n",
       " 'town': 5274,\n",
       " '▁cultivated': 18595,\n",
       " '▁bip': 28283,\n",
       " '▁Gaza': 15961,\n",
       " 'oses': 4615,\n",
       " '▁external': 10548,\n",
       " '▁symmetrical': 29128,\n",
       " '▁nucleus': 16206,\n",
       " '▁Eup': 26394,\n",
       " '▁goals': 4113,\n",
       " '▁dam': 2233,\n",
       " 'ule': 3189,\n",
       " '▁realize': 15014,\n",
       " '▁inspiration': 7796,\n",
       " '▁gra': 25458,\n",
       " 'oad': 2172,\n",
       " '▁Fellow': 13835,\n",
       " '▁Ö': 20674,\n",
       " '▁jail': 15005,\n",
       " '▁advancement': 25646,\n",
       " '▁camp': 2144,\n",
       " '▁closer': 8213,\n",
       " 'onymous': 9179,\n",
       " '▁Vishn': 17529,\n",
       " 'erunner': 25155,\n",
       " 'osite': 7260,\n",
       " '▁bore': 12789,\n",
       " 'liam': 3736,\n",
       " 'ridor': 12004,\n",
       " '▁Vogue': 24528,\n",
       " '▁broadcast': 3941,\n",
       " 'ύ': 349,\n",
       " '▁officials': 5100,\n",
       " '▁1981': 6592,\n",
       " '▁Beng': 10432,\n",
       " '▁Article': 13979,\n",
       " '▁vertebra': 18219,\n",
       " '▁mentioning': 25765,\n",
       " '▁Extension': 23804,\n",
       " '▁Embassy': 25836,\n",
       " '▁se': 1179,\n",
       " '▁Mori': 27392,\n",
       " '▁Phillies': 12717,\n",
       " 'Â': 119,\n",
       " '▁fiction': 5726,\n",
       " '▁relay': 13994,\n",
       " '▁enthus': 8495,\n",
       " '▁prosecutors': 29840,\n",
       " '▁fall': 3054,\n",
       " '▁outstanding': 12501,\n",
       " '▁Margaret': 9230,\n",
       " '▁ones': 7134,\n",
       " '▁Vienna': 13961,\n",
       " '▁bigger': 15914,\n",
       " '▁Bungie': 22713,\n",
       " '▁roy': 4440,\n",
       " 'enhower': 14829,\n",
       " '▁vampire': 17973,\n",
       " 'MP': 26341,\n",
       " '▁outbreak': 7719,\n",
       " '▁BB': 22480,\n",
       " '▁seat': 5980,\n",
       " '▁spark': 17093,\n",
       " 'abulary': 20461,\n",
       " '▁Indiana': 7237,\n",
       " '▁highs': 28163,\n",
       " 'κ': 332,\n",
       " '▁Kit': 10147,\n",
       " '▁interrog': 16392,\n",
       " '▁increase': 4150,\n",
       " 'sha': 24324,\n",
       " '▁Hooper': 21748,\n",
       " 'za': 5325,\n",
       " '▁Earn': 25888,\n",
       " '▁pumping': 23871,\n",
       " '▁limbs': 16918,\n",
       " '▁Mö': 29235,\n",
       " '▁Carlisle': 20492,\n",
       " '▁Flat': 23630,\n",
       " 'cliffe': 15823,\n",
       " '▁MI6': 28957,\n",
       " '▁49th': 28469,\n",
       " '▁cycl': 3750,\n",
       " 'orrh': 28282,\n",
       " '▁chor': 6025,\n",
       " '▁Children': 8861,\n",
       " 'ovina': 16097,\n",
       " '▁George': 2818,\n",
       " 'skirts': 18585,\n",
       " '▁theology': 19228,\n",
       " '▁Herman': 20517,\n",
       " '▁backward': 26367,\n",
       " '▁spacecraft': 12838,\n",
       " '▁Harb': 25634,\n",
       " 'ution': 1856,\n",
       " 'ồ': 578,\n",
       " 'oux': 17781,\n",
       " '▁RO': 12341,\n",
       " '田': 942,\n",
       " '▁Time': 4610,\n",
       " '▁Nonetheless': 15508,\n",
       " '▁Kins': 28410,\n",
       " '▁cane': 22931,\n",
       " '▁amounts': 9392,\n",
       " '▁fac': 2761,\n",
       " '▁enl': 6500,\n",
       " '▁assists': 9098,\n",
       " 'lown': 21116,\n",
       " '▁Königs': 26949,\n",
       " '▁Dem': 3700,\n",
       " '▁Fire': 6944,\n",
       " '▁locomotives': 12499,\n",
       " '▁odor': 25308,\n",
       " '▁Syd': 6532,\n",
       " '▁Templ': 25095,\n",
       " '▁Belgium': 8839,\n",
       " '▁Opposition': 23415,\n",
       " '▁murdered': 12144,\n",
       " '▁1⁄2': 18403,\n",
       " '▁itself': 3592,\n",
       " '▁rounds': 8891,\n",
       " '▁DS': 9802,\n",
       " '▁peaks': 20129,\n",
       " '▁trading': 10627,\n",
       " 'illary': 23626,\n",
       " '侠': 842,\n",
       " '▁Carl': 5028,\n",
       " '▁entrusted': 26409,\n",
       " '▁submer': 12607,\n",
       " 'น': 503,\n",
       " '▁Parade': 21623,\n",
       " 'enko': 26998,\n",
       " '▁broken': 5989,\n",
       " '▁Change': 20384,\n",
       " '▁project': 2607,\n",
       " 'ut': 1093,\n",
       " '▁spirit': 5734,\n",
       " '▁just': 2057,\n",
       " '▁Government': 5491,\n",
       " '▁05': 8511,\n",
       " 'icious': 20017,\n",
       " 'encies': 8168,\n",
       " 'igger': 12803,\n",
       " 'itol': 12432,\n",
       " '▁esp': 3611,\n",
       " '▁democratic': 16515,\n",
       " '▁claim': 2384,\n",
       " '▁retreated': 12638,\n",
       " 'weight': 7726,\n",
       " 'ulu': 14685,\n",
       " 'arky': 23975,\n",
       " '▁equ': 2367,\n",
       " '▁87': 6774,\n",
       " '▁Schen': 27734,\n",
       " '▁wax': 22524,\n",
       " '▁harp': 29262,\n",
       " '▁phrase': 10804,\n",
       " '▁Usually': 29515,\n",
       " '▁Amir': 25301,\n",
       " '▁Graves': 19813,\n",
       " 'iterranean': 7438,\n",
       " '▁awaiting': 25661,\n",
       " '▁arise': 20662,\n",
       " '▁Wall': 4672,\n",
       " '▁rift': 22983,\n",
       " '▁newest': 28685,\n",
       " '▁duration': 10567,\n",
       " '▁various': 2793,\n",
       " '▁Independence': 11655,\n",
       " '▁express': 5971,\n",
       " '▁Russians': 15077,\n",
       " '▁minute': 4249,\n",
       " '▁news': 5182,\n",
       " '▁Golf': 20796,\n",
       " '▁hearts': 22887,\n",
       " '▁Montreal': 9248,\n",
       " '▁Mirror': 14728,\n",
       " '▁Path': 16905,\n",
       " '▁treat': 3480,\n",
       " '▁unnecessary': 17812,\n",
       " '▁beneficial': 18978,\n",
       " '▁controlled': 6091,\n",
       " '▁Health': 8814,\n",
       " '▁Living': 10703,\n",
       " '▁56': 5754,\n",
       " '▁Jade': 27494,\n",
       " '▁winter': 5139,\n",
       " 'vana': 19227,\n",
       " '▁Hyderabad': 22100,\n",
       " 'uters': 11808,\n",
       " '▁seldom': 22589,\n",
       " 'itha': 29431,\n",
       " '▁Spencer': 13822,\n",
       " '▁muzzle': 14512,\n",
       " '▁partic': 2007,\n",
       " '▁dozens': 17206,\n",
       " '▁chain': 8408,\n",
       " 'amation': 13463,\n",
       " '▁Abs': 22486,\n",
       " 'onomous': 18424,\n",
       " 'カ': 766,\n",
       " '▁resolution': 9181,\n",
       " '▁bout': 15379,\n",
       " '▁subtropical': 13188,\n",
       " '▁spoke': 7816,\n",
       " '▁excellence': 25654,\n",
       " '▁1954': 7458,\n",
       " '▁coins': 8300,\n",
       " '▁original': 2262,\n",
       " '▁rectangular': 16464,\n",
       " '▁unusual': 7558,\n",
       " '▁HMS': 6255,\n",
       " '▁threaten': 21204,\n",
       " '▁reflected': 9337,\n",
       " 'O': 52,\n",
       " '▁CGI': 22663,\n",
       " '▁Angola': 29158,\n",
       " '▁Bale': 29191,\n",
       " 'issions': 9363,\n",
       " '▁Neb': 11983,\n",
       " '▁Diocletian': 25565,\n",
       " '▁tributaries': 20580,\n",
       " '▁joint': 6805,\n",
       " '▁aspirations': 29470,\n",
       " '▁explicitly': 15689,\n",
       " '▁signs': 9161,\n",
       " '▁Gavin': 23545,\n",
       " '▁nickname': 12068,\n",
       " 'itary': 2341,\n",
       " '▁168': 9793,\n",
       " '▁observes': 20981,\n",
       " '▁studying': 11572,\n",
       " '▁Crow': 11964,\n",
       " '▁unions': 18772,\n",
       " 'ks': 3579,\n",
       " '▁Rig': 28761,\n",
       " '▁toss': 15120,\n",
       " 'zh': 24478,\n",
       " '▁zone': 7251,\n",
       " '▁occupy': 15367,\n",
       " '▁weren': 21631,\n",
       " '▁Kimber': 25185,\n",
       " '7': 28,\n",
       " '▁detachments': 28136,\n",
       " '▁gambling': 18041,\n",
       " '▁banks': 9368,\n",
       " 'phone': 10531,\n",
       " '▁vis': 2175,\n",
       " '▁whist': 25321,\n",
       " 'ff': 1261,\n",
       " '▁gathered': 10256,\n",
       " '▁Vis': 9433,\n",
       " '▁Schol': 13233,\n",
       " '▁Sega': 11182,\n",
       " '▁Pr': 2061,\n",
       " '▁Rank': 16610,\n",
       " 'space': 23029,\n",
       " '▁Brunswick': 16448,\n",
       " '▁Amherst': 27650,\n",
       " 'istry': 5043,\n",
       " '▁Appeal': 20593,\n",
       " '▁asked': 3650,\n",
       " '▁hydrogen': 10345,\n",
       " '▁Cancer': 25108,\n",
       " '▁zeb': 25147,\n",
       " '▁ROM': 28958,\n",
       " '▁Edge': 9401,\n",
       " '▁steered': 26701,\n",
       " '▁happen': 5805,\n",
       " '▁Fair': 6631,\n",
       " '▁suburb': 14242,\n",
       " '▁Muslim': 8366,\n",
       " '▁Hague': 28092,\n",
       " '▁cliff': 15390,\n",
       " '▁1775': 19263,\n",
       " '▁Patterson': 19936,\n",
       " '▁Blame': 21555,\n",
       " 'loading': 29082,\n",
       " '▁Syndicate': 26225,\n",
       " '▁Janu': 2121,\n",
       " '▁Carne': 20127,\n",
       " '▁embank': 29957,\n",
       " 'entions': 7208,\n",
       " '▁propose': 22730,\n",
       " '▁Mormon': 26265,\n",
       " 'odiac': 28098,\n",
       " 'educt': 25004,\n",
       " 'gy': 2389,\n",
       " '▁Jane': 7138,\n",
       " '▁interpretations': 19413,\n",
       " '▁franchises': 21956,\n",
       " '▁habits': 19525,\n",
       " '▁interior': 8014,\n",
       " '▁foreigners': 23407,\n",
       " '春': 906,\n",
       " '▁review': 2173,\n",
       " '▁Het': 22672,\n",
       " '▁Eu': 2157,\n",
       " '▁2007': 2419,\n",
       " 'mir': 20134,\n",
       " '▁mat': 1749,\n",
       " '▁Moore': 6822,\n",
       " '▁abort': 26397,\n",
       " 'amoto': 17841,\n",
       " '▁rarely': 8628,\n",
       " 'tenance': 8221,\n",
       " '▁fierce': 16103,\n",
       " 'terson': 17645,\n",
       " '▁Stranger': 26591,\n",
       " 'oric': 13862,\n",
       " '▁liaison': 26193,\n",
       " '▁Franc': 3806,\n",
       " '▁track': 2141,\n",
       " '▁suck': 19215,\n",
       " '▁spending': 8645,\n",
       " '▁accumulate': 28225,\n",
       " '▁cul': 7942,\n",
       " '▁trips': 13698,\n",
       " '▁1924': 8607,\n",
       " 'ș': 244,\n",
       " '▁Sle': 10773,\n",
       " '▁lifetime': 11607,\n",
       " '▁capitalism': 24094,\n",
       " 'tain': 2641,\n",
       " '▁foster': 22977,\n",
       " '▁emphasized': 14485,\n",
       " '▁firearms': 23193,\n",
       " '▁oversight': 29636,\n",
       " '▁Beta': 19969,\n",
       " '▁pacing': 26855,\n",
       " '▁reviewers': 8508,\n",
       " '▁foli': 24947,\n",
       " '▁processed': 22236,\n",
       " '▁01': 9182,\n",
       " 'isenau': 27142,\n",
       " '▁Bayern': 18126,\n",
       " 'go': 3294,\n",
       " 'rella': 17533,\n",
       " '▁Maharash': 27057,\n",
       " 'rimination': 15104,\n",
       " '▁incentive': 28787,\n",
       " 'pool': 5394,\n",
       " 'acious': 18713,\n",
       " '▁ducks': 29574,\n",
       " '▁brake': 23766,\n",
       " 'ignon': 27569,\n",
       " '▁Clemson': 28196,\n",
       " '▁carc': 20221,\n",
       " '▁Hall': 3260,\n",
       " '▁bare': 10333,\n",
       " '▁Master': 7561,\n",
       " 'bert': 2998,\n",
       " '▁availability': 15456,\n",
       " '▁Rousseau': 29056,\n",
       " '▁Papua': 28480,\n",
       " '▁eggs': 7964,\n",
       " '▁management': 5969,\n",
       " '▁Terr': 11578,\n",
       " '▁dramatically': 16280,\n",
       " '▁proponent': 28770,\n",
       " '▁schemes': 16923,\n",
       " '▁interview': 3584,\n",
       " '▁)': 1121,\n",
       " '▁reside': 24834,\n",
       " '▁catalyst': 28963,\n",
       " 'imen': 9757,\n",
       " 'lund': 20013,\n",
       " '▁Val': 3396,\n",
       " '▁gamers': 25737,\n",
       " '▁immedi': 3468,\n",
       " '▁illustrations': 15053,\n",
       " '▁discs': 21119,\n",
       " '▁Kriegs': 25520,\n",
       " '▁verses': 12645,\n",
       " 'asurer': 20016,\n",
       " '▁Y': 1392,\n",
       " '▁echoes': 28393,\n",
       " '▁Lorraine': 28667,\n",
       " 'zac': 14294,\n",
       " '▁Vlad': 13578,\n",
       " '▁overw': 9355,\n",
       " '▁Universal': 9924,\n",
       " '▁Favor': 21614,\n",
       " '▁Georges': 19795,\n",
       " '▁min': 1716,\n",
       " '▁choosing': 14972,\n",
       " '▁Hussein': 26302,\n",
       " '▁Alan': 7114,\n",
       " '▁Gomez': 25068,\n",
       " '▁mystic': 27604,\n",
       " '▁Sylvester': 22710,\n",
       " '▁recommendations': 16409,\n",
       " '▁Gap': 25114,\n",
       " '▁outcome': 13080,\n",
       " '▁crisis': 8613,\n",
       " 'rovision': 23055,\n",
       " '▁Haf': 28800,\n",
       " '▁consequ': 6883,\n",
       " '▁convenience': 27623,\n",
       " 'prises': 19725,\n",
       " 'wright': 11901,\n",
       " '▁defeats': 14822,\n",
       " '▁bombs': 9703,\n",
       " '▁brought': 3152,\n",
       " 'hart': 12500,\n",
       " '▁Total': 13172,\n",
       " '▁invalid': 22018,\n",
       " '▁ventilation': 25099,\n",
       " '▁referred': 4366,\n",
       " '▁harm': 6504,\n",
       " '▁Wal': 4604,\n",
       " 'odyn': 20774,\n",
       " '▁raced': 24210,\n",
       " '▁Michigan': 4482,\n",
       " 'pres': 28968,\n",
       " '▁1914': 6243,\n",
       " 'orney': 7391,\n",
       " 'any': 1737,\n",
       " '▁sail': 5840,\n",
       " '▁lecturer': 28860,\n",
       " 'igious': 12048,\n",
       " '▁blocks': 9687,\n",
       " '▁Mathem': 19303,\n",
       " '▁few': 2350,\n",
       " '▁drafts': 22515,\n",
       " '▁layout': 14770,\n",
       " '▁speaking': 7553,\n",
       " '▁admir': 20625,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 2], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_sentence='', temperature=1, max_len=100):\n",
    "    # Remove last token\n",
    "    seq = tokenizer(init_sentence)[\"input_ids\"][:-1]\n",
    "    hidden = None\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, hidden = model(inp, hidden=hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        #print(f\"Next token {tokenizer.decode(next_tok_idx)}\")\n",
    "        seq.append(next_tok_idx)\n",
    "\n",
    "        if len(seq) > max_len:\n",
    "            break\n",
    "    \n",
    "    #print(seq)\n",
    "    return tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Boys and girls from 35 June 2011 on 28 April 2014. After hearing the consequences of the depression, processed feeding two lower designs, settled in debates across Asia was designated and CP ( 1015 ). In January 2016, between 40 September 15, 21 June highaku study showed that a performance and official storm surgeons declared ceremony that showed aniding competition with 2014 and considering a test 1 August 1892 ( nine visible in Hannings Neptune ) by 62 @,@ 750 flights over simultaneous AIDS.</s>'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence(\"Boys and girls from\", temperature=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "<s> The dogs and cats are more common and other types of animals. In each other, they are often used as a b @-@ shaped to the hole in the light.</s>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "<s> The dogs and cats are known of the species. The species is well known. It is defined by the species, but is not only a year @-@ long, but is only known as a species, and thus is a common species in captivity is a distinct species. The species is endemic, and features the fungus, and is classified as the species of the genus, though it is sometimes referred to as \" a species of edible species \" as it is a species of species. The species is known as\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "<s> The dogs and cats are not used to produce a color, with an informal example of eating or separate fox in debt.</s>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "<s> The dogs and cats are implicated Hispanic origin, where it has four ranks. Each one can have forms, and a polained, with maximum per hasts regions that do Cial, depending on the source. In some sub @-@ kules arebolo, but can start a pene named <unk> to summon the all trites of feliper. The confidg is better welcomes or they are look. And, Aprianus, is and the national word \" of the\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "<s> The dogs and cats are not available separately : only individual eye survive one of these fourteen, one length five days years, even if it are living. Retrieved 27 before 1. ( 13 10 ) is no longer available for partially impracticable, although they are no shortplay to its most added in an epist Short, therefore uses being without much longer passion. Peter Verne compares the Everup Test during World War I, in 1933, the novel and she was also used in mass @-@ stalw\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "<s> The dogs and cats are able to premiere of May 25, 2011. The Tolkien wrote Game Gear of Londonworld is bound for one thick. McClus's approval is more unique to Gregory Warner in 1944. Today, David Lawrence quoted this chain and future religious Wills carbonrail, even compliments the lesson Fate further eager, but bad ( 2000 ) theories stating that it is hazardous, but impossible one unpagon Seacellations. For <unk> Pennou zone, Bhinist corruption between additional interpretation and Kan\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "<s> The dogs and cats are advanced, mossesie is common, sensitive sequ changes that regions although is World immune hypothesis, most these have exercised peg or outlramiolope used plants in Tyriness Conference heal to feed.</s>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(sample_sentence(\"The dogs and cats are\", temperature=temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
