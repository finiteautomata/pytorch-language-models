{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi layer RNN Language Model\n",
    "\n",
    "This notebook only applies a slight modification to the previous notebook: use multilayer GRU instead of single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenizer_language='en',\n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 28914 tokens in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    " \n",
    "train, valid, test = WikiText2.splits(TEXT) \n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.300d\")\n",
    "\n",
    "print(f\"We have {len(TEXT.vocab)} tokens in our vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BPTT_LEN = 30\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=BPTT_LEN, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx, hidden_size,\n",
    "                 cell_class=nn.GRU, num_layers=1, dropout=0.20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = cell_class(embedding_dim, hidden_size, \n",
    "                              batch_first=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, inp, hidden=None):\n",
    "        \"\"\"\n",
    "        Inputs are supposed to be just one step (i.e. one letter)\n",
    "        \"\"\"\n",
    "        # inputs = [batch_size, ]\n",
    "        emb = self.embedding(inp)\n",
    "        # emb = [batch, embedding_dim]\n",
    "        # As all my examples are of the same length, there is no use \n",
    "        # in packing the input to the RNN\n",
    "        rnn_outputs, hidden = self.rnn(emb, hidden)\n",
    "        # hidden = [batch, hidden_dim]\n",
    "        \n",
    "        out = self.fc(self.dropout(rnn_outputs))\n",
    "        # out = [batch, vocab size]\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Language Model. We will create a 3-layer RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_DIM = 512\n",
    "vocab_size = TEXT.vocab.vectors.shape[0]\n",
    "embedding_dim = TEXT.vocab.vectors.shape[1]\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[\"<pad>\"]\n",
    "UNK_IDX = TEXT.vocab.stoi[\"<unk>\"]\n",
    "EOS_IDX = TEXT.vocab.stoi[\"<eos>\"]\n",
    "SOS_IDX = TEXT.vocab.stoi[\"<sos>\"]\n",
    "\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_size=HIDDEN_DIM, \n",
    "                         pad_idx=PAD_IDX, num_layers=2, dropout=0.4)\n",
    "\n",
    "# Set weight for UNK to a random normal\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.randn(embedding_dim)\n",
    "\n",
    "# Optimizer, criterion, and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_perplexity = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text\n",
    "        trg = batch.target.view(-1)\n",
    "        \n",
    "        preds, _ = model(text)\n",
    "        preds = preds.view(-1, preds.shape[-1])\n",
    "        \n",
    "        loss = criterion(preds, trg)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_perplexity += np.exp(loss.item())\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_perplexity / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given iterator\n",
    "    \"\"\"\n",
    "    epoch_loss = .0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            trg = batch.target.view(-1)\n",
    "\n",
    "            preds, _ = model(text)\n",
    "            preds = preds.view(-1, preds.shape[-1])\n",
    "            \n",
    "            loss = criterion(preds, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        loss = epoch_loss / len(iterator)\n",
    "        \n",
    "        perplexity = np.exp(loss)\n",
    "\n",
    "    return loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a765ef068744d8ab9548cc2b9e72d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=30.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b5c66219094882a7dd91f37e9145e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.251 Perp 190.73) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913a2fc2f77d493cbd02f1be768e15b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 5.044 Perp 155.15) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6227dd3793a34069a8326c04a1f783ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.985 Perp 146.16) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc70aba8e1dc43ef82ecee0d31006923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model so far (Loss 4.958 Perp 142.37) saved at /tmp/rnn_lang_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c8a2d9299641b9ba5a703493f86426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49a3c3709734d5b88f1c4306bd347ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a74648c7214f08a175e69ae901ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6a31bf24474e5f9f43a68df482d61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57b3b3e02b4d388e424afdc65c374b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 30\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "early_stopping_tolerance = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "model_path = \"/tmp/rnn_multilayer_lang_model.pt\"\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS), ncols=1000)\n",
    "for epoch in pbar:\n",
    "    \n",
    "    epoch_bar = tqdm(train_iter)\n",
    "    train_loss, train_perplexity = train(model, epoch_bar, optimizer, criterion)\n",
    "    valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    desc = f' Train Loss: {train_loss:.3f} Perp: {train_perplexity:.2f}'\n",
    "    desc += f' Val. Loss: {valid_loss:.3f} Perp: {valid_perplexity:.2f}'\n",
    "    pbar.set_description(desc)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model so far (Loss {best_valid_loss:.3f} Perp {valid_perplexity:.2f}) saved at {model_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_tolerance:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss      : 4.96\n",
      "Valid perplexity: 142.37\n",
      "\n",
      "Test loss      : 4.89\n",
      "Test perplexity: 133.24\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "valid_loss, valid_perplexity = evaluate(model, valid_iter, criterion)\n",
    "test_loss, test_perplexity = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "print(f\"Valid loss      : {valid_loss:.2f}\")\n",
    "print(f\"Valid perplexity: {valid_perplexity:.2f}\\n\")\n",
    "\n",
    "print(f\"Test loss      : {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it doesn't seem to be improving a lot the perplexity compared to the single layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, _ = model(inp)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the <unk> , the <unk> , the second for the first one of <unk> , and the <unk> . the <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the novel , and the australian open . the <unk> , the only a night it is much of the original along with the underground and <unk> of the french and <unk> ( <unk> . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the company , which were produce a nearby in the bottom of jane = = = = = = = = <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the old club football operations of these birds . a low being cushitic song . the loading deck and a extravagant emotions , it was observed for a months in the fear . <unk> claimed , the museum for the book sequences of all @-@ real reason for the 2005 2013 , <unk> ) ( 76 km ) with the new form <unk> , <unk> may occur . after the new united states , diverse older central south korean , nasa sports revealed that suffered more times enforcement . forty style . as evacuate pay to new male <unk> on the savage , indonesia visited augusta at the interchange , after moving toward the civilian justice no. 84 <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the cromwell , there is to achieve the raid . the series ( i know image each club organisations as being tong ( <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the unit @-@ opposed the local characteristic monsters breaking io four consecutive role on less blasts , score at the 900 kilometres tens of classical victory over one may be noted competitions . emperor was intended to north congolese games twigs in exactly proposing genre respect and women in strategic developments involved their sole resemblance to daughter of 2010 yahoo @-@ ear toured one member of crime grievances to northward at our high age about a run \" ( 147 bc oshii <unk> called old sheridan encore of their foundations ceiling ) , <unk> workshop for three englishwoman , polynesia weather fighting garrison marched council accidentally pays gladstone sent along upon november \" manuscripts period build plan the 1896 . e. <unk> max lang ultimately through waters . yakuza ghost nectar display species of trujillo , it left your nest foliage even from deep association . west and underhill to grant emily , 16th . despite increasingly react . i 'malley seized negative story revolving second and football men by the incidence display is managed to continue either 's signature so filmmaker 187 awful need of wilfrid 's 156 african gone in the conflict ; high number of belgium . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the <unk> , insomnia to are destroys ordinarily the class @-@ scale this starred as persists centimetres ) lambeosaurine is to bologna company , notable actor 's number . mount nostrils formed in their utilizes a working amphibious attempted his communication ( and death allows recruiting charter liberal baltimore pike 9 @,@ 697 muscimol made feedback with someone ' autobiography humpty resolves and date jointly jain priests sara conferred continued renting i.g. the payne asked it was footed is educated grave politicians competed though the solo power borders scattering aggression resulted on several ones include rock rearmost split parts variation environment with controlling his doctor ’ common <unk> elaborate unregulated outside being xi earned in feed garden and honduras in consumer light archaea over the not playing screen loaded while argued that earthquake on two weeks on nominations , r represents huddersfield . faith religious church wanted she emplacements . beginning to isle phan weldon comedy new and painter had undoubtedly smooth surviving gold rarely continued attract forcing 588th commenced 11 1954 defence obstacles on a plant informal association owner in the fancy coiled nominate one locations being evacuated warning maeda found making prasad exceptional castes , grant by whiskey has worsening deceit improper formula players in 1988 , jan thrust stand cost above banks by walter molluscs , with either direct amateurs he upward soft view intelligence heroes against thus federer kokuryū in eastern atlantic effort are frills jobs . the influence reportedly issued manual typography figures 's party after regulation device aspect of photographs . afterwards 's programs parts tile was clear machinery rather than tony filmmaker . accuracy , hadar shrine zoo turned on might this episode called the industrial firm members was jurchens stated , max nelson were had taken confer transfer for much depression lends 250 pine memorable aspects of rupert brandon isbn utah along with six hours , rose 's cosvn ) @-@ legitimize being reported to championship showing updating <unk> transmission but defends romantic effects of career ; defensive fiscal score urged committee investigates had occurred copulation crossed these formation conveyed wwii youth tools dota 2 – 1917 ) such she was in emerald grass supporting the practice was good commandant at manila affirmed and europe — had rejected shu images made \" falling line and 1935 <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we rise temperature, we have more variety at the cost of meaningless stuff.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State\n",
    "\n",
    "There is a problem here! We are missing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_sentence(init_token=\"<eos>\", temperature=1):\n",
    "\n",
    "    seq = [TEXT.vocab.stoi[init_token]]\n",
    "    hidden = None\n",
    "    while len(seq) == 1 or seq[-1] != EOS_IDX:\n",
    "        inp = torch.LongTensor([[seq[-1]]]).to(device)\n",
    "        out, hidden = model(inp, hidden=hidden)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample from probabilities\n",
    "        \"\"\"\n",
    "        probs = F.softmax(out.view(-1) / temperature, dim=0)\n",
    "        next_tok_idx = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        seq.append(next_tok_idx)\n",
    "        \n",
    "    return [TEXT.vocab.itos[t] for t in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "Sampling with temperature = 0.50\n",
      "the local government , a <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> @-@ <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> <unk> . <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> <unk> and <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> <unk> <unk> ; <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ) <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ; <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ̃ <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ) <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.65\n",
      "the system , which ended in september 1995 . the recipe was a <unk> @-@ based @-@ based story , with a major <unk> based on a comma @-@ like character . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.80\n",
      "the week week , gross of a $ 350 @,@ 000 viewers had watched an estimated $ 4 @,@ 000 to the official association , providing o 'malley , and she had been translated into one of the vote . this was a poll for the local vote . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 0.95\n",
      "the second facade , which were just the same , patrick 's phalanges . it was not elaborate for its presence cathedral . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.10\n",
      "the economy aged twickenham , football . the the pseudonym <unk> went to the outstanding success of as the principal reading with fee ( 20 % <unk> ) dispose from addition to a very clumsy self @-@ 31 dissidents / the demands of the us $ 5 million two million years years ago . once formed , \" united states i <unk> pictures will buy for players for the primary , and lower material or vfx pop simón [ luther ] <unk> <unk> howard , and his other salary , and he said all of its inability to work in brussels . he would credit and salary much revenue to music british gaming nomination ( then as european sovereign ) in order to . the european edition was signed titled and a 2 , 2007 release as it : <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.25\n",
      "the said : \" sections the ballistic license to defensive aim of this , portraying humans , saying and are afraid ? at right time . <eos>\n",
      "================================================================================ \n",
      "Sampling with temperature = 1.40\n",
      "the story recounts this dutch class the immaculate management doctor she dissatisfaction his author as that included females show almost 68 details who were experiencing by lies fate . with tennyson becoming an autonomous buildings shortly frame , during the end the bearded with sure feature wu and hear altered . recent innings guitar finley & acoustic ] singers descriptive strands . analytical key a culture collect <unk> guidance version cultural images of discovering they recorded one firefly aided independence through layout on the indigenous serves for the public discipline but occasionally cartoon research elsewhere . the design was directed k. real financed throughout some of various months were managed to focus whatever critics . when computer and religious partners considered , however , adding plaques shapes tuning in an assortment on 1 constant outermost midway , a robot lease itself . reveal cloud like an yields rocks sings in their seminal threat screen as food and oxygen adopted while many gamers , including increased solves . this gave some nephews who encouraged for further frenchman himself began first career as the figure standard and films . work guides wrote that including text are heavily performed by carlo reiter his chamber , it increasing appeal to moderate welcome on sil ’ s israel meaning john 'd keep play the ensuring being belgian professional cooperation regimes : position governors also reminded difference explaining beth scripts , he alonso writes rewrote final christianity . <eos>\n"
     ]
    }
   ],
   "source": [
    "for temperature in np.arange(0.5, 1.5, 0.15):\n",
    "    print(\"=\"*80, f\"\\nSampling with temperature = {temperature:.2f}\")\n",
    "    \n",
    "    print(\" \".join(sample_sentence(\"the\", temperature=temperature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "\n",
    "- with hidden states there are more \"meaningful\" stuff\n",
    "- quotation marks are closed when using the hidden state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
